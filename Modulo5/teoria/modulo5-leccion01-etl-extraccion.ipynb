{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ETL-(Extract,-Transform,-Load)\" data-toc-modified-id=\"ETL-(Extract,-Transform,-Load)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ETL (Extract, Transform, Load)</a></span></li><li><span><a href=\"#ELT-(Extract,-Load,-Transform)\" data-toc-modified-id=\"ELT-(Extract,-Load,-Transform)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ELT (Extract, Load, Transform)</a></span></li><li><span><a href=\"#Diferencias-entre-ETL-y-ELT\" data-toc-modified-id=\"Diferencias-entre-ETL-y-ELT-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Diferencias entre ETL y ELT</a></span></li><li><span><a href=\"#Caso-práctico\" data-toc-modified-id=\"Caso-práctico-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Caso práctico</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](https://github.com/Hack-io-Data/Imagenes/blob/main/01-LogosHackio/logo_amarillo@4x.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL (Extract, Transform, Load)\n",
    "\n",
    "ETL es un proceso fundamental en el ámbito del análisis de datos y gestión de información. Usamos los procesos ETL para integrar datos de diversas fuentes en un repositorio centralizado, limpiar y estandarizar los datos para eliminar errores e inconsistencias, y convertirlos en formatos útiles y enriquecidos para análisis posteriores. Esto asegura que los datos sean adecuados para herramientas de BI y análisis, y que estén centralizados en un sistema optimizado para consultas rápidas y eficientes, mejorando la velocidad y capacidad de respuesta de los análisis. Este proceso consta de tres pasos principales:\n",
    "\n",
    "1. **Extract (Extracción):**  Este paso implica la recolección de datos de diversas fuentes heterogéneas. Estas fuentes pueden incluir bases de datos relacionales, archivos planos (como CSV y JSON), APIs, Web Scrapping,  sistemas ERP, sistemas CRM, y más. El gran desafío en esta fase es que en algunos casos, no nos encontraremos con los datos homogenéos ya que llegan de distintas fuentes de datos.\n",
    "\n",
    "2. **Transform (Transformación):** Una vez extraídos los datos, se limpian, enriquecen y transforman para adecuarlos al formato requerido para el análisis. Esto incluye la eliminación de valores nulos, corrección de errores, normalización, agregación o la aplicación de reglas de negocio. Esta fase puede incluir tareas como unir tablas, derivar nuevas columnas, aplicar funciones de agregación, y más. \n",
    "  \n",
    "\n",
    "3. **Load (Carga):** El paso final es cargar los datos transformados en un sistema de almacenamiento, como un data warehouse (p. ej., Amazon Redshift, Google BigQuery) o una base de datos analítica (p. ej., PostgreSQL, Snowflake).\n",
    "\n",
    "![etl](https://github.com/Hack-io-Data/Imagenes/blob/main/02-Imagenes/ETL/ETL.png?raw=true)\n",
    "\n",
    "**¿Para qué sirven?**\n",
    "\n",
    "El proceso ETL se utiliza para consolidar datos de múltiples fuentes en un repositorio centralizado. Esto permite:\n",
    "\n",
    "- **Análisis Integral:** Ofrecer una vista unificada de los datos para análisis más completos.\n",
    "\n",
    "- **Decisiones Informadas:** Facilitar la toma de decisiones basada en datos precisos y actualizados.\n",
    "\n",
    "- **Operaciones Optimizadas:** Mejorar la eficiencia operativa mediante la centralización de datos.\n",
    "\n",
    "**Algunos ejemplos prácticos de ETL a los que nos podríamos enfrentar en nuestro día a día**\n",
    "\n",
    "-  **Análisis de Ventas en una Empresa de Retail**: Una empresa de retail quiere analizar las ventas de sus tiendas físicas y online para mejorar su estrategia de marketing y gestión de inventarios. ¿Cómo sería el proceso de ETL?\n",
    "\n",
    "  - Extracción: Se extraen datos de ventas diarias de diversas fuentes, incluyendo bases de datos de punto de venta (POS) en tiendas físicas, plataformas de e-commerce, y sistemas de gestión de inventarios.\n",
    "\n",
    "  - Transformación: Los datos se limpian y estandarizan para corregir errores y discrepancias (por ejemplo, nombres de productos inconsistentes). Se agregan campos calculados como el margen de beneficio y se combinan las ventas online y físicas.\n",
    "\n",
    "  - Carga: Los datos transformados se cargan en un data warehouse centralizado, como Amazon Redshift, donde se pueden realizar análisis avanzados y generar informes de ventas detallados.\n",
    "\n",
    "\n",
    "- **Pipeline de ETL para Análisis Financiero en una Institución Bancaria** : Un banco quiere consolidar datos financieros de diferentes sistemas internos y externos para cumplir con las regulaciones y realizar análisis de riesgos. Los pasos a seguir en este caso serían: \n",
    "\n",
    "  - Extracción: Se extraen datos de transacciones financieras de sistemas internos del banco, datos de mercado de proveedores externos y datos históricos de archivos almacenados en sistemas legacy.\n",
    "\n",
    "  - Transformación: Los datos se limpian y transforman para asegurar que cumplen con los estándares regulatorios (por ejemplo, formateo de fechas, eliminación de datos duplicados). Se realizan agregaciones y cálculos financieros como tasas de interés promedio y volúmenes de transacciones.\n",
    "\n",
    "  - Carga: Los datos transformados se cargan en un data warehouse como Snowflake, donde se pueden realizar análisis de riesgos, generar informes financieros y asegurar el cumplimiento regulatorio.\n",
    "\n",
    "\n",
    "# ELT (Extract, Load, Transform)\n",
    "\n",
    "ELT es un enfoque similar al ETL, pero la secuencia de pasos difiere, en este caso:\n",
    "\n",
    "1. **Extract (Extracción):** Los datos se extraen de las fuentes de datos originales.\n",
    "\n",
    "2. **Load (Carga):** Los datos extraídos se cargan directamente en un sistema de almacenamiento, como un data lake o un data warehouse.\n",
    "\n",
    "3. **Transform (Transformación):** Las transformaciones se realizan dentro del sistema de almacenamiento utilizando su capacidad de procesamiento.\n",
    "\n",
    "![elt](https://github.com/Hack-io-Data/Imagenes/blob/main/02-Imagenes/ETL/ELT.png?raw=true)\n",
    "\n",
    "El proceso ELT se utiliza en entornos donde los sistemas de almacenamiento tienen suficiente capacidad y poder de procesamiento para manejar grandes volúmenes de datos y realizar transformaciones complejas directamente en el almacenamiento. El objetivo principal del ELT es aprovechar la capacidad de procesamiento del sistema de almacenamiento para realizar transformaciones de datos más complejas y a gran escala, permitiendo una mayor flexibilidad y escalabilidad en el manejo de datos.\n",
    "\n",
    "**Planteando los ejemplos que vimos en el apartado de ETL, el proceso en caso de que quisieramos hacer una ELT sería:**\n",
    "\n",
    "- **Pipeline de ELT para Análisis de Ventas en una Empresa de Retail**: Una empresa de retail quiere analizar las ventas de sus tiendas físicas y online para mejorar su estrategia de marketing y gestión de inventarios. Los pasos serían: \n",
    "\n",
    "- Extracción: Se extraen datos de ventas diarias de diversas fuentes, incluyendo bases de datos de punto de venta (POS) en tiendas físicas, plataformas de e-commerce, y sistemas de gestión de inventarios.\n",
    "\n",
    "- Carga: Los datos extraídos se cargan directamente en un data lake o data warehouse, como Amazon Redshift.\n",
    "\n",
    "- Transformación: Dentro del data warehouse, los datos se limpian y estandarizan para corregir errores y discrepancias (por ejemplo, nombres de productos inconsistentes). Se agregan campos calculados como el margen de beneficio y se combinan las ventas online y físicas utilizando SQL y otras herramientas de procesamiento dentro del data warehouse.\n",
    "\n",
    "\n",
    "- **Pipeline de ELT para Análisis Financiero en una Institución Bancaria**: Un banco quiere consolidar datos financieros de diferentes sistemas internos y externos para cumplir con las regulaciones y realizar análisis de riesgos. Los pasos serían: \n",
    "\n",
    "  - Extracción: Se extraen datos de transacciones financieras de sistemas internos del banco, datos de mercado de proveedores externos y datos históricos de archivos almacenados en sistemas legacy.\n",
    "\n",
    "  - Carga: Los datos extraídos se cargan directamente en un data warehouse, como Snowflake.\n",
    "\n",
    "  - Transformación: Dentro del data warehouse, los datos se limpian y transforman para asegurar que cumplen con los estándares regulatorios (por ejemplo, formateo de fechas, eliminación de datos duplicados). Se realizan agregaciones y cálculos financieros como tasas de interés promedio y volúmenes de transacciones utilizando las capacidades de procesamiento de Snowflake.\n",
    "\n",
    "\n",
    "# Diferencias entre ETL y ELT\n",
    "\n",
    "A la vista de estos dos nuevos conceptos, podríamos pensar ¿qué diferencia hay entre ellos? Os dejamos aquí una tabla resumen con las principales diferencias\n",
    "\n",
    "| **Criterio**          | **ETL**                                                                 | **ELT**                                                                                         |\n",
    "|-----------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| **Secuencia de Pasos**| Primero se extraen los datos, luego se transforman y finalmente se cargan en el sistema de almacenamiento. | Primero se extraen los datos, luego se cargan en el sistema de almacenamiento y finalmente se transforman. |\n",
    "| **Uso de Recursos**   | Requiere herramientas de procesamiento intermedio para la transformación antes de la carga. | Aprovecha la capacidad de procesamiento del sistema de almacenamiento para realizar transformaciones. |\n",
    "| **Escalabilidad**     | Puede ser menos eficiente con grandes volúmenes de datos debido a la necesidad de transformar los datos antes de la carga. | Más adecuado para grandes volúmenes de datos, ya que las transformaciones se realizan después de la carga, utilizando la infraestructura del almacenamiento. |\n",
    "| **Flexibilidad**      | Menos flexible para realizar cambios en las transformaciones, ya que estas se realizan antes de la carga. | Más flexible, ya que las transformaciones se pueden ajustar después de que los datos están en el sistema de almacenamiento. |\n",
    "| **Tecnología de Soporte** | Tradicionalmente utilizado con data warehouses. | Más común en entornos de big data y data lakes, donde se necesita manejar y procesar grandes volúmenes de datos de manera eficiente. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso práctico\n",
    "\n",
    "En este notebook, realizaremos una serie de acciones que nos permitirán extraer información de la página web del Ministerio de Sanidad y organizar los archivos descargados en nuestro sistema local. Este proceso implicará la configuración del entorno para la navegación web automatizada, la descarga de datos, la manipulación de archivos y su organización final. Aquí tenemos una guía paso a paso de lo que haremos:\n",
    "\n",
    "- Paso 1: Configurar las opciones del driver: Configuraremos el **driver** del navegador que controlará la automatización web. Recordemos que las opciones del driver nos permiten especificar preferencias como la ubicación de descarga de archivos, la supresión de mensajes emergentes y otros comportamientos del navegador.\n",
    "\n",
    "- Paso 2: Sacar el número de años de los que vamos a extraer los datos: Extraeremos el número de años que tenemos en la página web. \n",
    "\n",
    "- Paso 3: Descargar los archivos: Descargaremos los archivos correspondientes a las estadísticas de centros sanitarios. Esta etapa involucra seleccionar opciones en un formulario web y hacer clic en botones para iniciar la descarga.\n",
    "\n",
    "- Paso 4: Cambiar los nombres de los archivos: Una vez que los archivos están descargados, les asignaremos nombres más intuitivos para facilitar su identificación y manejo.\n",
    "\n",
    "- Paso 5: Descomprimir los archivos: Descomprimiremos los archivos zip descargados para poder trabajar con los datos contenidos en ellos.\n",
    "\n",
    "- Paso 6: Ordenar y reorganizar los archivos y carpetas: Finalmente, ordenaremos y reorganizaremos los archivos y carpetas según una estructura lógica para facilitar su acceso y análisis.\n",
    "\n",
    "\n",
    "![extraccion](https://github.com/Hack-io-Data/Imagenes/blob/main/02-Imagenes/ETL/Esquema_ETL_Extraccion.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #40B5AD; padding: 10px; border-left: 6px solid #000080; color: black; border-radius: 10px;\">\n",
    "\n",
    "A lo largo de las lecciones de esta semana abordaremos un problema: \n",
    "\n",
    "**Análisis Financiero de Centros Especializados**\n",
    "\n",
    "Evaluar la eficiencia y sostenibilidad financiera de los centros especializados, para ello necesitaremos identificar patrones y tendencias en los ingresos y gastos, y proporcionar recomendaciones para mejorar la gestión financiera y operativa de los centros. Para ello, tendremos que: \n",
    "\n",
    "- Realizar un análisis descriptivo para entender la distribución de los gastos e ingresos.\n",
    "\n",
    "- Analizar la evolución de los gastos e ingresos a lo largo del tiempo.\n",
    "\n",
    "- Identificar tendencias estacionales y cambios a largo plazo en los patrones de gastos e ingresos.\n",
    "\n",
    "- Analizar la relación entre diferentes tipos de gastos e ingresos.\n",
    "\n",
    "- Identificar qué tipos de gastos están más correlacionados con altos ingresos y cuáles no.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium para establecer la configuración del driver\n",
    "# -----------------------------------------------------------------------\n",
    "from selenium import webdriver\n",
    "\n",
    "# Para generar una barra de proceso en los bucles for\n",
    "# -----------------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Para trabajar con ficheros\n",
    "# -----------------------------------------------------------------------\n",
    "import os\n",
    "\n",
    "# Importación de las funciones creadas en nuestro archivo de soporte\n",
    "# -----------------------------------------------------------------------\n",
    "from src import soporte_extraccion as se\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definimos la configuración del driver y sacamos el número de años de los que tenemos información**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo primero que vamos a hacer es configurar nuestras preferencias del navegador para el driver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "# establacemos las preferencias que queremos\n",
    "prefs = {\n",
    "    \"download.default_directory\": \"/Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescargados\",  # AQUÍ CADA UNO TENDREMOS QUE PONER LA RUTA QUE QUERAMOS PARA QUE SE GUARDEN LOS ARCHIVOS DESCARGADOS\n",
    "    \"download.prompt_for_download\": False,   # desactiva el diálogo que Chrome normalmente muestra para pedir confirmación del usuario antes de descargar un archivo\n",
    "    \"directory_upgrade\": True,    # hace que Chrome actualice el directorio de descarga predeterminado a la nueva ubicación especificada por download.default_directory si esta ha cambiado.\n",
    "    \"safebrowsing.enabled\": True # protegemos al driver de sitios web peligrosos\n",
    "}\n",
    "\n",
    "# configuramos el navegador Chrome con preferencias específicas definidas en el diccionario prefs\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "chrome_options.add_argument(\"--headless\")  # evita que se abrán las ventanas del driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de años que tenemos en la pagina web del ministerio de sanidad son: 13\n"
     ]
    }
   ],
   "source": [
    "# definimos la url sobre la que vamos a trabajar para sacar el número de años que tenemos en la web\n",
    "url_inicio = \"https://www.sanidad.gob.es/estadisticas/microdatos.do\"\n",
    "\n",
    "# llamamos a la función que tenemos en el archivo de soporte para sacar el número de años\n",
    "num_años = se.sacar_anios(chrome_options, url_inicio)\n",
    "print(f\"El número de años que tenemos en la pagina web del ministerio de sanidad son: {num_años}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descargamos los datos para cada uno de los años**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:05<00:00,  5.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# pasamos a la siguiente fase, en la que descargaremos los datos de todos los años\n",
    "# llamamos a la función de descargar los datos que tenemos en el soporte\n",
    "for i in tqdm(range(num_años -1)):\n",
    "    se.descargar_datos_anio(i, chrome_options, url_inicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cambiamos el nombre de los archivos y descomprimimos los archivos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 2018.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018\n",
      "Archivo 2019.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019\n",
      "Archivo 2021.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2021\n",
      "Archivo 2020.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020\n",
      "Archivo 2012.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2012\n",
      "Archivo 2013.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2013\n",
      "Archivo 2011.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2011\n",
      "Archivo 2010.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2010\n",
      "Archivo 2014.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2014\n",
      "Archivo 2015.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2015\n",
      "Archivo 2017.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017\n",
      "Archivo 2016.zip extraído en /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016\n"
     ]
    }
   ],
   "source": [
    "# definimos la ruta \n",
    "ruta = \"/Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescargados\"\n",
    "\n",
    "# llamamos a la función para cambiar los nombres de los ficheros\n",
    "se.cambiar_nombre_archivos(ruta)\n",
    "\n",
    "# descomprimimos los archivos zip descargados previamente\n",
    "se.descomprimir_zip(\"/Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescargados\", \"/Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reorganizamos las carpetas del proyecto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la ruta donde tenemos los archivos con los que vamos a trabajar\n",
    "ruta = \"/Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos\"\n",
    "\n",
    "# iteramos por la lista de carpetas que tenemos en la ruta definida previamente\n",
    "for carpeta in os.listdir(ruta):\n",
    "\n",
    "   # construimos la ruta para acceder a cada una de las carpetas de datos que tenemos\n",
    "    ruta_carpeta = os.path.join(ruta, carpeta)\n",
    "\n",
    "    # evaluamos si dentro de la carpeta hay otra carpeta, o si hay ficheros\n",
    "    if se.buscar_archivos_y_carpetas(ruta_carpeta) == \"carpetas\":\n",
    "\n",
    "        # en caso de que haya carpetas, mover los archivos a la carpeta principal\n",
    "        se.mover_archivos_a_principal(ruta_carpeta)\n",
    "    \n",
    "    # si no son carpetas no hacemos nada\n",
    "    else:\n",
    "        pass     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
