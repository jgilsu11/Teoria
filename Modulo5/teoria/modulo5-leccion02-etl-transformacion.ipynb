{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://github.com/Hack-io-Data/Imagenes/blob/main/01-LogosHackio/logo_amarillo@4x.png?raw=true\" alt=\"esquema\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ETL-(Extract,-Transform,-Load)\" data-toc-modified-id=\"ETL-(Extract,-Transform,-Load)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ETL (Extract, Transform, Load)</a></span></li><li><span><a href=\"#ELT-(Extract,-Load,-Transform)\" data-toc-modified-id=\"ELT-(Extract,-Load,-Transform)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ELT (Extract, Load, Transform)</a></span></li><li><span><a href=\"#Diferencias-entre-ETL-y-ELT\" data-toc-modified-id=\"Diferencias-entre-ETL-y-ELT-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Diferencias entre ETL y ELT</a></span></li><li><span><a href=\"#Caso-práctico\" data-toc-modified-id=\"Caso-práctico-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Caso práctico</a></span></li><li><span><a href=\"#Descripción-de-los-datos-que-vamos-a-usar\" data-toc-modified-id=\"Descripción-de-los-datos-que-vamos-a-usar-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Descripción de los datos que vamos a usar</a></span><ul class=\"toc-item\"><li><span><a href=\"#Datos-de-Ingresos\" data-toc-modified-id=\"Datos-de-Ingresos-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Datos de Ingresos</a></span></li><li><span><a href=\"#Datos-de-Gastos\" data-toc-modified-id=\"Datos-de-Gastos-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Datos de Gastos</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL (Extract, Transform, Load)\n",
    "\n",
    "ETL es un proceso fundamental en el ámbito del análisis de datos y gestión de información. Usamos los procesos ETL para integrar datos de diversas fuentes en un repositorio centralizado, limpiar y estandarizar los datos para eliminar errores e inconsistencias, y convertirlos en formatos útiles y enriquecidos para análisis posteriores. Esto asegura que los datos sean adecuados para herramientas de BI y análisis, y que estén centralizados en un sistema optimizado para consultas rápidas y eficientes, mejorando la velocidad y capacidad de respuesta de los análisis. Este proceso consta de tres pasos principales:\n",
    "\n",
    "1. **Extract (Extracción):**  Este paso implica la recolección de datos de diversas fuentes heterogéneas. Estas fuentes pueden incluir bases de datos relacionales, archivos planos (como CSV y JSON), APIs, Web Scrapping,  sistemas ERP, sistemas CRM, y más. El gran desafío en esta fase es que en algunos casos, no nos encontraremos con los datos homogenéos ya que llegan de distintas fuentes de datos.\n",
    "\n",
    "2. **Transform (Transformación):** Una vez extraídos los datos, se limpian, enriquecen y transforman para adecuarlos al formato requerido para el análisis. Esto incluye la eliminación de valores nulos, corrección de errores, normalización, agregación o la aplicación de reglas de negocio. Esta fase puede incluir tareas como unir tablas, derivar nuevas columnas, aplicar funciones de agregación, y más. \n",
    "  \n",
    "\n",
    "3. **Load (Carga):** El paso final es cargar los datos transformados en un sistema de almacenamiento, como un data warehouse (p. ej., Amazon Redshift, Google BigQuery) o una base de datos analítica (p. ej., PostgreSQL, Snowflake).\n",
    "\n",
    "![etl](https://github.com/Hack-io-Data/Imagenes/blob/main/02-Imagenes/ETL/ETL.png?raw=true)\n",
    "\n",
    "**¿Para qué sirven?**\n",
    "\n",
    "El proceso ETL se utiliza para consolidar datos de múltiples fuentes en un repositorio centralizado. Esto permite:\n",
    "\n",
    "- **Análisis Integral:** Ofrecer una vista unificada de los datos para análisis más completos.\n",
    "\n",
    "- **Decisiones Informadas:** Facilitar la toma de decisiones basada en datos precisos y actualizados.\n",
    "\n",
    "- **Operaciones Optimizadas:** Mejorar la eficiencia operativa mediante la centralización de datos.\n",
    "\n",
    "**Algunos ejemplos prácticos de ETL a los que nos podríamos enfrentar en nuestro día a día**\n",
    "\n",
    "-  **Análisis de Ventas en una Empresa de Retail**: Una empresa de retail quiere analizar las ventas de sus tiendas físicas y online para mejorar su estrategia de marketing y gestión de inventarios. ¿Cómo sería el proceso de ETL?\n",
    "\n",
    "  - Extracción: Se extraen datos de ventas diarias de diversas fuentes, incluyendo bases de datos de punto de venta (POS) en tiendas físicas, plataformas de e-commerce, y sistemas de gestión de inventarios.\n",
    "\n",
    "  - Transformación: Los datos se limpian y estandarizan para corregir errores y discrepancias (por ejemplo, nombres de productos inconsistentes). Se agregan campos calculados como el margen de beneficio y se combinan las ventas online y físicas.\n",
    "\n",
    "  - Carga: Los datos transformados se cargan en un data warehouse centralizado, como Amazon Redshift, donde se pueden realizar análisis avanzados y generar informes de ventas detallados.\n",
    "\n",
    "\n",
    "- **Pipeline de ETL para Análisis Financiero en una Institución Bancaria** : Un banco quiere consolidar datos financieros de diferentes sistemas internos y externos para cumplir con las regulaciones y realizar análisis de riesgos. Los pasos a seguir en este caso serían: \n",
    "\n",
    "  - Extracción: Se extraen datos de transacciones financieras de sistemas internos del banco, datos de mercado de proveedores externos y datos históricos de archivos almacenados en sistemas legacy.\n",
    "\n",
    "  - Transformación: Los datos se limpian y transforman para asegurar que cumplen con los estándares regulatorios (por ejemplo, formateo de fechas, eliminación de datos duplicados). Se realizan agregaciones y cálculos financieros como tasas de interés promedio y volúmenes de transacciones.\n",
    "\n",
    "  - Carga: Los datos transformados se cargan en un data warehouse como Snowflake, donde se pueden realizar análisis de riesgos, generar informes financieros y asegurar el cumplimiento regulatorio.\n",
    "\n",
    "\n",
    "# ELT (Extract, Load, Transform)\n",
    "\n",
    "ELT es un enfoque similar al ETL, pero la secuencia de pasos difiere, en este caso:\n",
    "\n",
    "1. **Extract (Extracción):** Los datos se extraen de las fuentes de datos originales.\n",
    "\n",
    "2. **Load (Carga):** Los datos extraídos se cargan directamente en un sistema de almacenamiento, como un data lake o un data warehouse.\n",
    "\n",
    "3. **Transform (Transformación):** Las transformaciones se realizan dentro del sistema de almacenamiento utilizando su capacidad de procesamiento.\n",
    "\n",
    "![elt](https://github.com/Hack-io-Data/Imagenes/blob/main/02-Imagenes/ETL/ELT.png?raw=true)\n",
    "\n",
    "El proceso ELT se utiliza en entornos donde los sistemas de almacenamiento tienen suficiente capacidad y poder de procesamiento para manejar grandes volúmenes de datos y realizar transformaciones complejas directamente en el almacenamiento. El objetivo principal del ELT es aprovechar la capacidad de procesamiento del sistema de almacenamiento para realizar transformaciones de datos más complejas y a gran escala, permitiendo una mayor flexibilidad y escalabilidad en el manejo de datos.\n",
    "\n",
    "**Planteando los ejemplos que vimos en el apartado de ETL, el proceso en caso de que quisieramos hacer una ELT sería:**\n",
    "\n",
    "- **Pipeline de ELT para Análisis de Ventas en una Empresa de Retail**: Una empresa de retail quiere analizar las ventas de sus tiendas físicas y online para mejorar su estrategia de marketing y gestión de inventarios. Los pasos serían: \n",
    "\n",
    "- Extracción: Se extraen datos de ventas diarias de diversas fuentes, incluyendo bases de datos de punto de venta (POS) en tiendas físicas, plataformas de e-commerce, y sistemas de gestión de inventarios.\n",
    "\n",
    "- Carga: Los datos extraídos se cargan directamente en un data lake o data warehouse, como Amazon Redshift.\n",
    "\n",
    "- Transformación: Dentro del data warehouse, los datos se limpian y estandarizan para corregir errores y discrepancias (por ejemplo, nombres de productos inconsistentes). Se agregan campos calculados como el margen de beneficio y se combinan las ventas online y físicas utilizando SQL y otras herramientas de procesamiento dentro del data warehouse.\n",
    "\n",
    "\n",
    "- **Pipeline de ELT para Análisis Financiero en una Institución Bancaria**: Un banco quiere consolidar datos financieros de diferentes sistemas internos y externos para cumplir con las regulaciones y realizar análisis de riesgos. Los pasos serían: \n",
    "\n",
    "  - Extracción: Se extraen datos de transacciones financieras de sistemas internos del banco, datos de mercado de proveedores externos y datos históricos de archivos almacenados en sistemas legacy.\n",
    "\n",
    "  - Carga: Los datos extraídos se cargan directamente en un data warehouse, como Snowflake.\n",
    "\n",
    "  - Transformación: Dentro del data warehouse, los datos se limpian y transforman para asegurar que cumplen con los estándares regulatorios (por ejemplo, formateo de fechas, eliminación de datos duplicados). Se realizan agregaciones y cálculos financieros como tasas de interés promedio y volúmenes de transacciones utilizando las capacidades de procesamiento de Snowflake.\n",
    "\n",
    "\n",
    "# Diferencias entre ETL y ELT\n",
    "\n",
    "A la vista de estos dos nuevos conceptos, podríamos pensar ¿qué diferencia hay entre ellos? Os dejamos aquí una tabla resumen con las principales diferencias\n",
    "\n",
    "| **Criterio**          | **ETL**                                                                 | **ELT**                                                                                         |\n",
    "|-----------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| **Secuencia de Pasos**| Primero se extraen los datos, luego se transforman y finalmente se cargan en el sistema de almacenamiento. | Primero se extraen los datos, luego se cargan en el sistema de almacenamiento y finalmente se transforman. |\n",
    "| **Uso de Recursos**   | Requiere herramientas de procesamiento intermedio para la transformación antes de la carga. | Aprovecha la capacidad de procesamiento del sistema de almacenamiento para realizar transformaciones. |\n",
    "| **Escalabilidad**     | Puede ser menos eficiente con grandes volúmenes de datos debido a la necesidad de transformar los datos antes de la carga. | Más adecuado para grandes volúmenes de datos, ya que las transformaciones se realizan después de la carga, utilizando la infraestructura del almacenamiento. |\n",
    "| **Flexibilidad**      | Menos flexible para realizar cambios en las transformaciones, ya que estas se realizan antes de la carga. | Más flexible, ya que las transformaciones se pueden ajustar después de que los datos están en el sistema de almacenamiento. |\n",
    "| **Tecnología de Soporte** | Tradicionalmente utilizado con data warehouses. | Más común en entornos de big data y data lakes, donde se necesita manejar y procesar grandes volúmenes de datos de manera eficiente. |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso práctico\n",
    "\n",
    "Los pasos que seguiremos a lo largo de esta lección son: \n",
    "\n",
    "\n",
    "- Paso 1: Apertura y Conversión de Archivos: Empezaremos por abrir los archivos de Access utilizando herramientas adecuadas para la manipulación de bases de datos Access. Esto nos permitirá extraer las tablas de datos necesarias para nuestro análisis.\n",
    "\n",
    "- Paso 2: Conversión de archivos Access a .txt: Una vez abiertos los archivos Access, convertiremos los datos a archivos de texto (.txt). Esta conversión facilita la manipulación de los datos en las siguientes etapas del proceso ETL, permitiendo un acceso más flexible a la información.\n",
    "\n",
    "- Paso 3: Extracción de datos de ingresos y gastos: De los archivos de texto obtenidos en los pasos anteriores y de los que ya teníamos en formato .txt, extraeremos las tablas relevantes que contienen información sobre ingresos y gastos hospitalarios. Este paso implica leer los archivos de texto y seleccionar las columnas y filas que contienen los datos necesarios. Resumamos lo que vamos a hacer en cada tipo de conjunto de datos (separando por gastos e ingresos): \n",
    "\n",
    "   - En el caso de los datos de gastos: \n",
    "\n",
    "      - Renombrado de columnas: Eliminaremos los códigos numéricos de los nombres de las columnas. Este paso hará que los nombres de las columnas sean más claros y directos, facilitando su interpretación y análisis.\n",
    "\n",
    "   - En el caso de los datos de ingresos: \n",
    "\n",
    "      -  Filtrado de columnas por tipo de hospitalización: Revisaremos las columnas disponibles y eliminaremos aquellas que no correspondan a \"HospDom\", \"Hospital\" y \"hospDia\". Este paso reducirá la complejidad del conjunto de datos, enfocándonos en los tipos de hospitalización más relevantes para nuestro análisis.\n",
    "\n",
    "      - Creación de conjuntos de datos por tipo de hospitalización: Dividiremos el conjunto de datos de ingresos en tres conjuntos separados, uno para cada tipo de hospitalización (\"HospDom\", \"Hospital\", \"hospDia\"). Esto permitirá un análisis más específico y detallado de cada modalidad de hospitalización. Estos conjuntos se unirán y crearemos una columna nueva que identifique el tipo de hospitalización. \n",
    "\n",
    "      - Filtrado de columnas por tipo de ingresos: Eliminaremos las columnas que no correspondan a los tipos de ingresos 700, 701 y 702. Al enfocarnos en estos tipos específicos de ingresos, simplificaremos el conjunto de datos y nos centraremos en las categorías más significativas para nuestro estudio.\n",
    "\n",
    "      - Renombrado de Columnas: Cambiaremos los nombres de las columnas, eliminando los códigos numéricos y simplificando los nombres para que sean más intuitivos y fáciles de entender. Esto mejorará la legibilidad y manejabilidad de los datos.\n",
    "\n",
    "- Paso 4: Guardado de datos en archivos CSV por año: Los datos extraídos se guardarán en archivos CSV, organizados por año. Esto permitirá una mejor gestión y análisis temporal de la información, facilitando el seguimiento de las tendencias anuales en ingresos y gastos.\n",
    "\n",
    "- Paso 5: Unificación de datos de ingresos y gastos: Finalmente, uniremos los datos transformados de ingresos y gastos en un único conjunto de datos. \n",
    "\n",
    "![diagrama](https://github.com/Hack-io-Data/Imagenes/blob/main/02-Imagenes/ETL/Esquema_ETL_Transformacion.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #40B5AD; padding: 10px; border-left: 6px solid #000080; color: black; border-radius: 10px;\">\n",
    "\n",
    "A lo largo de las lecciones de esta semana abordaremos un problema: \n",
    "\n",
    "**Análisis Financiero de Centros Especializados**\n",
    "\n",
    "Evaluar la eficiencia y sostenibilidad financiera de los centros especializados, para ello necesitaremos identificar patrones y tendencias en los ingresos y gastos, y proporcionar recomendaciones para mejorar la gestión financiera y operativa de los centros. Para ello, tendremos que: \n",
    "\n",
    "- Realizar un análisis descriptivo para entender la distribución de los gastos e ingresos.\n",
    "\n",
    "- Analizar la evolución de los gastos e ingresos a lo largo del tiempo.\n",
    "\n",
    "- Identificar tendencias estacionales y cambios a largo plazo en los patrones de gastos e ingresos.\n",
    "\n",
    "- Analizar la relación entre diferentes tipos de gastos e ingresos.\n",
    "\n",
    "- Identificar qué tipos de gastos están más correlacionados con altos ingresos y cuáles no.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripción de los datos que vamos a usar\n",
    "\n",
    "## Datos de Ingresos\n",
    "\n",
    "Tenemos muchísima información en este conjunto de datos de ingresos, y para simplificar el ejercicio que estamos haciendo eliminaremos algunas columnas. Si nos fijamos, hay varias columnas que tienen un código númerico, seguido de \"Urgencia\", \"HospDom\", \"Hospital\", \"consulExter\", \"CMA\" y \"hospDia\". ¿Qué significan estas columnas? Hacen refrencia al tipo de hospitalización. En concreto, tenemos los siguientes valores:\n",
    "\n",
    "- `Urgencia`: Ingresos en urgencias\n",
    "\n",
    "- `HospDom`: Ingresos en hospitalizaciones domiciliarias.\n",
    "\n",
    "- `Hospital`: Ingresos por hospitalización.\n",
    "\n",
    "- `consulExter`: Ingresos por consultas externas.\n",
    "\n",
    "- `CMA`: Ingresos por Cirugía Mayor Ambulatoria.\n",
    "\n",
    "- `hospDia`: Ingresos por Hospital de Día.\n",
    "\n",
    "En este caso, nos vamos a quedar solo con las columnas que hagan referencia HospDom, Hospital y hospDia y eliminaremos el resto de las columnas listadas. Para que sean más fáciles los siguientes pasos, crearemos tres conjuntos de datos, uno para cada tipo de hopitalización.\n",
    "\n",
    "\n",
    "Dentro de estos conjuntos de datos de gastos, tenemos una serie de códigos numéricos que nos están indicando el tipo de ingresos, los cuales son poco intuitivos. El significado de cada uno de los códigos es: \n",
    "\n",
    "- **70:**  Ingresos por prestación de servicios en diferentes modalidades (hospitalización, consultas externas, Cirugía Mayor Ambulatoria, Hospital de Día, urgencias, hospitalización a domicilio).\n",
    "\n",
    "- **700:** Ingresos por prestación de servicios a particulares en diferentes modalidades.\n",
    "\n",
    "- **701:** Ingresos por servicios a través de aseguradoras privadas en diferentes modalidades. Incluye:\n",
    "\n",
    "     - **701_1:** Seguros de asistencia sanitaria y enfermedad.\n",
    "\n",
    "     - **701_2:** Accidentes de tráfico.\n",
    "\n",
    "- **702:** Ingresos por servicios prestados a través de mutuas de accidentes de trabajo y enfermedades profesionales en diferentes modalidades.\n",
    "\n",
    "- **704:** Ingresos por servicios prestados a través de organismos del Sistema Nacional de Salud en diferentes modalidades.\n",
    "\n",
    "- **705:** Ingresos por financiación directa de servicios de salud en diferentes modalidades. Incluye:\n",
    "\n",
    "     - **705_1:** Financiación directa de servicios de salud y otras entidades públicas.\n",
    "\n",
    "     - **705_2:** Financiación directa de aseguradoras privadas y mutuas de accidentes de trabajo.\n",
    "\n",
    "- **706:** Ingresos por servicios prestados a través de otras entidades y organismos públicos en diferentes modalidades.\n",
    "\n",
    "- **708_XXX:** Bonificaciones aplicadas a ingresos en diferentes modalidades.\n",
    "\n",
    "Al igual que hicimos con el tipo de hospitalizaciones, filtrando algunos de los tipos, en el caso del tipo de ingresos haremos lo mismo. En este caso, nos quedaremos con:\n",
    "\n",
    "- Ingresos por prestación de servicios (700).\n",
    "\n",
    "- Ingresos por servicios a través de aseguradoras privadas (701).\n",
    "\n",
    "- Ingresos por servicios prestados a través de mutuas de accidentes de trabajo y enfermedades profesionales (702).\n",
    "\n",
    "Por último, cambiaremos el nombre de las columnas, ya que tenemos los códigos (que no son intuitivos) y la palabra 'Hospital', para eso vamos a usar el método `rename`. \n",
    "\n",
    "Para más detalle de estos datos, tenéis [este](RecursosAdicionales/ExplicacionDatosIngresos.md) anexo. \n",
    "\n",
    "\n",
    "## Datos de Gastos\n",
    "\n",
    "En este caso, la limpieza será más sencilla, ya que nos quedaremos con todas las columnas, y lo único que haremos será quitar el código numérico del nombre de las columnas. \n",
    "\n",
    "\n",
    "Para más detalle de estos datos, tenéis [este](RecursosAdicionales/ExplicacionDatosGastos.md) anexo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para trabajar con ficheros\n",
    "# -----------------------------------------------------------------------\n",
    "import os\n",
    "\n",
    "# Importación de las funciones creadas en nuestro archivo de soporte\n",
    "# -----------------------------------------------------------------------\n",
    "from src import soporte_transformacion as st\n",
    "\n",
    "# Para trabajar con los DataFrames\n",
    "# -----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Para poner barras progreso\n",
    "# -----------------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Configuración\n",
    "# -----------------------------------------------------------------------\n",
    "pd.set_option('display.max_columns', None) # para poder visualizar todas las columnas de los DataFrames\n",
    "\n",
    "# Ignorar warings\n",
    "# -----------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convertimos los access a archivos '.txt'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #030303; padding: 10px; border-left: 6px solid #FE0101; color: white; border-radius: 10px;\">\n",
    "\n",
    "La primera vez que ejecutéis este código, puede que os de un error similar al que véis a continuación: \n",
    "\n",
    "```python\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'mdb-tables'\n",
    "```\n",
    "\n",
    "Para poder resolverlo, debemos instalar <strong>mdbtools</strong>. Para eso, debemos escribir el siguiente comando en la terminal:\n",
    "\n",
    "```bash\n",
    "brew install mdbtools # esto puede llevar un ratillo\n",
    "```\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_01_FILIACION anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_02_OFERTA_ASISTENCIAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_03_DOTACION_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_04_DOTACION_TECNOLOGICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_05_PERSONAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_07_FORMACION anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_08_SERVICIOS_CONTRATADOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_09_ACTIVI_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_10_ACTIVIDAD_QUIRURGICA anonimizada.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5/12 [00:00<00:00,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_11_ACTIVIDAD_OBSTETRICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_12_HOSPITAL_DIA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_13_HOSPITALIZACION_DOMICILIO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_14_ACTIVIDAD_CONSULTAS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_15_ACTIVIDAD_URGENCIAS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_16_ACTIVIDAD_DIAGNOSTICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_17_OTRA_ACTIVIDAD anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_18_REG_ECONOMICO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_19_GASTOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_20_INGRESOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_21_INVERSIONES anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2017/C1_06_RESTPERS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_01_FILIACION anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_02_OFERTA_ASISTENCIAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_03_DOTACION_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_04_DOTACION_TECNOLOGICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_05_PERSONAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_06_RESTPERS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_07_FORMACION anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_09_ACTIVI_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_08_SERVICIOS_CONTRATADOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_10_ACTIVIDAD_QUIRURGICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_11_ACTIVIDAD_OBSTETRICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_12_HOSPITAL_DIA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_13_HOSPITALIZACION_DOMICILIO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_14_ACTIVIDAD_CONSULTAS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_15_ACTIVIDAD_URGENCIAS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_16_ACTIVIDAD_DIAGNOSTICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_17_OTRA_ACTIVIDAD anonimizada.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7/12 [00:00<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_18_REG_ECONOMICO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_19_GASTOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_20_INGRESOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2019/C1_21_INVERSIONES anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_02_OFERTA_ASISTENCIAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_03_DOTACION_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_04_DOTACION_TECNOLOGICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_05_PERSONAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_06_RESTPERS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_07_FORMACION anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_08_SERVICIOS_CONTRATADOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_09_ACTIVI_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_10_ACTIVIDAD_QUIRURGICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_11_ACTIVIDAD_OBSTETRICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_12_HOSPITAL_DIA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_13_HOSPITALIZACION_DOMICILIO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_14_ACTIVIDAD_CONSULTAS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_15_ACTIVIDAD_URGENCIAS anonimizada.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [00:01<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_16_ACTIVIDAD_DIAGNOSTICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_17_OTRA_ACTIVIDAD anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_18_REG_ECONOMICO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_19_GASTOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_20_INGRESOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_21_INVERSIONES anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2020/C1_01_FILIACION anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_02_OFERTA_ASISTENCIAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_03_DOTACION_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_04_DOTACION_TECNOLOGICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_06_RESTPERS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_07_FORMACION anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_09_ACTIVI_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_10_ACTIVIDAD_QUIRURGICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_11_ACTIVIDAD_OBSTETRICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_12_HOSPITAL_DIA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_13_HOSPITALIZACION_DOMICILIO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_14_ACTIVIDAD_CONSULTAS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_15_ACTIVIDAD_URGENCIAS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_16_ACTIVIDAD_DIAGNOSTICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_17_OTRA_ACTIVIDAD anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_18_REG_ECONOMICO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_19_GASTOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_20_INGRESOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_21_INVERSIONES anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_01_FILIACION anonimizada.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10/12 [00:01<00:00,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_08_SERVICIOS_CONTRATADOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2018/C1_05_PERSONAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_03_DOTACION_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_04_DOTACION_TECNOLOGICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_06_RESTPERS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_07_FORMACION anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_08_SERVICIOS_CONTRATADOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_10_ACTIVIDAD_QUIRURGICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_11_ACTIVIDAD_OBSTETRICA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_12_HOSPITAL_DIA anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_13_HOSPITALIZACION_DOMICILIO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_14_ACTIVIDAD_CONSULTAS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_15_ACTIVIDAD_URGENCIAS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_16_ACTIVIDAD_DIAGNOSTICA anonimizada.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_17_OTRA_ACTIVIDAD anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_18_REG_ECONOMICO anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_19_GASTOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_20_INGRESOS anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_21_INVERSIONES anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_01_FILIACION anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_05_PERSONAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_09_ACTIVI_HOSPITAL anonimizada.txt\n",
      "Datos exportados a /Users/ana.garcia/Documents/Contenido/DataScience/ETL/DatosDescomprimidos/2016/C1_02_OFERTA_ASISTENCIAL anonimizada.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# definimos la ruta donde tenemos los datos que queremos descomprimir\n",
    "carpeta_descomprimidos  = \"/Users/ana.garcia/Documents/Contenido/DataScience/Modulo5/datos/DatosDescomprimidos\"\n",
    "\n",
    "# iteramos por cada una de las carpetas de los años que tenemos de archivos descomprimidos\n",
    "for carpeta in tqdm(os.listdir(carpeta_descomprimidos)):\n",
    "\n",
    "    # definimos la ruta de cada una de las carpetas que tenemos, la de cada una de los años\n",
    "    ruta_subcarpeta = os.path.join(carpeta_descomprimidos, carpeta)\n",
    "\n",
    "    # evaluamos si en la carpeta hay solo un archivo, en caso de que sea así, hay un archivo de access\n",
    "    if len(os.listdir(ruta_subcarpeta)) == 1:\n",
    "\n",
    "        # creamos la ruta del archivo de access del que vamos a extraer las tablas\n",
    "        ruta_fichero = os.path.join(ruta_subcarpeta, os.listdir(ruta_subcarpeta)[0])\n",
    "  \n",
    "        # sacamos el nombre de las tablas de los archivos de access\n",
    "        tablas = st.sacar_nombres_tablas_access(ruta_fichero)\n",
    "        #print(f\"Tablas en el archivo de Access: {tablas}\")\n",
    "    \n",
    "        # convertimos las tablas de access a txt llamando a la función que tenemos en el archivo de soporte\n",
    "        st.convertir_access_txt(tablas=tablas, ruta = ruta_fichero, ruta_destino = ruta_subcarpeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformamos los datos de ingresos y gastos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 102.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Vamos a definir las rutas de las carpetas de donde sacamos los archivos y donde queremos almacenar los resutlados después de toda la transformación\n",
    "ruta_origen = \"/Users/ana.garcia/Documents/Contenido/DataScience/Modulo5/datos/DatosDescomprimidos\"\n",
    "ruta_destino = \"/Users/ana.garcia/Documents/Contenido/DataScience/Modulo5/datos/DatosFinales\"\n",
    "\n",
    "# iteramos por la ruta de origen, para sacar todas las carpetas que tenemos (las de los diferentes años)\n",
    "for carpeta in tqdm(os.listdir(ruta_origen)):\n",
    "\n",
    "    # creamos la ruta, para poder acceder a los datos de cada uno de los años\n",
    "    ruta_años = os.path.join(ruta_origen, carpeta)\n",
    "\n",
    "    # accedemos a todos los ficheros de cada una de las carpetas de años que tenemos\n",
    "    for archivo in (os.listdir(ruta_años)):\n",
    "\n",
    "        # seleccionamos solo los ficheros de ingresos\n",
    "        if \"ingresos\" in archivo.lower():\n",
    "\n",
    "            # definimos la ruta para abrir cada uno de los ficheros y realizar las transformaciones\n",
    "            ruta_fichero_ingresos = os.path.join(ruta_años, archivo)\n",
    "\n",
    "            # aplicamos la función que tenemos en el archivo de soporte\n",
    "            st.crear_df_ingresos(ruta_origen = ruta_fichero_ingresos, ruta_destino = ruta_destino, anio = carpeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 137.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# iteramos por la ruta de origen, para sacar todas las carpetas que tenemos (las de los diferentes años)\n",
    "for carpeta in tqdm(os.listdir(ruta_origen)):\n",
    "\n",
    "    # creamos la ruta, para poder acceder a los datos de cada uno de los años\n",
    "    ruta_años = os.path.join(ruta_origen, carpeta)\n",
    "\n",
    "    # accedemos a todos los ficheros de cada una de las carpetas de años que tenemos\n",
    "    for archivo in (os.listdir(ruta_años)):\n",
    "\n",
    "          # seleccionamos solo los ficheros de gastos\n",
    "         if \"gastos\" in archivo.lower():\n",
    "\n",
    "              # definimos la ruta para abrir cada uno de los ficheros y realizar las transformaciones \n",
    "              ruta_fichero_gastos = os.path.join(ruta_años, archivo)\n",
    "\n",
    "              # aplicamos la función que tenemos en el archivo de soporte\n",
    "              st.crear_df_gastos(ruta_fichero_gastos, ruta_destino = ruta_destino, anio = carpeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unimos los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un patrón para buscar todos los archivos que son de tipo 'csv'  y contienen 'gastos' en este ejemplo\n",
    "patron_gastos = \"*gastos*.csv\"  \n",
    "patron_ingresos = \"*ingresos*.csv\" \n",
    "ruta = '/Users/ana.garcia/Documents/Contenido/DataScience/Modulo5/datos/DatosFinales'\n",
    "\n",
    "df_gastos_final = st.leer_todos_archivos(patron = patron_gastos, ruta_origen = ruta, nombre_fichero = \"finalG\")\n",
    "df_ingresos_final = st.leer_todos_archivos(patron = patron_ingresos, ruta_origen = ruta, nombre_fichero = \"finalI\")\n",
    "\n",
    "# unimos todos los datos\n",
    "st.unir_datos(df_ingresos_final, df_gastos_final, ruta_destino = \"/Users/ana.garcia/Documents/Contenido/DataScience/Modulo5/datos/DatosFinales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
