{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://github.com/Hack-io-Data/Imagenes/blob/main/01-LogosHackio/logo_naranja@4x.png?raw=true\" alt=\"esquema\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introducción-a-de-Web-Scraping\" data-toc-modified-id=\"Introducción-a-de-Web-Scraping-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introducción a de Web Scraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#¿Qué-es-el-Web-Scraping?\" data-toc-modified-id=\"¿Qué-es-el-Web-Scraping?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>¿Qué es el <strong>Web Scraping</strong>?</a></span></li></ul></li><li><span><a href=\"#Introducción-a-HTML\" data-toc-modified-id=\"Introducción-a-HTML-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introducción a HTML</a></span><ul class=\"toc-item\"><li><span><a href=\"#¿Qué-es-HTML?\" data-toc-modified-id=\"¿Qué-es-HTML?-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>¿Qué es <strong>HTML</strong>?</a></span></li></ul></li><li><span><a href=\"#Selectores-CSS\" data-toc-modified-id=\"Selectores-CSS-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Selectores CSS</a></span><ul class=\"toc-item\"><li><span><a href=\"#¿-Qué-son-los-selectores-CSS?\" data-toc-modified-id=\"¿-Qué-son-los-selectores-CSS?-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>¿ Qué son los selectores CSS?</a></span></li></ul></li><li><span><a href=\"#Introducción-a-Beautiful-Soup\" data-toc-modified-id=\"Introducción-a-Beautiful-Soup-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Introducción a Beautiful Soup</a></span><ul class=\"toc-item\"><li><span><a href=\"#¿Qué-es--Beautiful-Soup?\" data-toc-modified-id=\"¿Qué-es--Beautiful-Soup?-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>¿Qué es  <strong>Beautiful Soup</strong>?</a></span></li></ul></li><li><span><a href=\"#Métodos-importantes-de-Beautiful-Soup\" data-toc-modified-id=\"Métodos-importantes-de-Beautiful-Soup-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Métodos importantes de <em>Beautiful Soup</em></a></span></li><li><span><a href=\"#Biblioteca-requests\" data-toc-modified-id=\"Biblioteca-requests-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Biblioteca requests</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importaciones\" data-toc-modified-id=\"Importaciones-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Importaciones</a></span></li></ul></li><li><span><a href=\"#Extracción-de-datos-de-wikipedia:\" data-toc-modified-id=\"Extracción-de-datos-de-wikipedia:-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Extracción de datos de wikipedia:</a></span></li><li><span><a href=\"#Creación-de-una-función:\" data-toc-modified-id=\"Creación-de-una-función:-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Creación de una función:</a></span></li><li><span><a href=\"#Extracción-de-datos-de-zapatillas:\" data-toc-modified-id=\"Extracción-de-datos-de-zapatillas:-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Extracción de datos de zapatillas:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Petición-y-guardado-del-código-HTML:\" data-toc-modified-id=\"Petición-y-guardado-del-código-HTML:-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Petición y guardado del código HTML:</a></span></li><li><span><a href=\"#Descripción-del-producto\" data-toc-modified-id=\"Descripción-del-producto-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Descripción del producto</a></span></li><li><span><a href=\"#Precio-producto\" data-toc-modified-id=\"Precio-producto-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Precio producto</a></span><ul class=\"toc-item\"><li><span><a href=\"#Limpieza-de-precios\" data-toc-modified-id=\"Limpieza-de-precios-9.3.1\"><span class=\"toc-item-num\">9.3.1&nbsp;&nbsp;</span>Limpieza de precios</a></span></li></ul></li><li><span><a href=\"#Gastos-de-envío\" data-toc-modified-id=\"Gastos-de-envío-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Gastos de envío</a></span><ul class=\"toc-item\"><li><span><a href=\"#Limpieza-de-gastos-de-envío\" data-toc-modified-id=\"Limpieza-de-gastos-de-envío-9.4.1\"><span class=\"toc-item-num\">9.4.1&nbsp;&nbsp;</span>Limpieza de gastos de envío</a></span></li></ul></li><li><span><a href=\"#Tipo-de-vendedor:\" data-toc-modified-id=\"Tipo-de-vendedor:-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>Tipo de vendedor:</a></span></li><li><span><a href=\"#Imagen-del-producto\" data-toc-modified-id=\"Imagen-del-producto-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>Imagen del producto</a></span></li><li><span><a href=\"#Creación-de-una-función\" data-toc-modified-id=\"Creación-de-una-función-9.7\"><span class=\"toc-item-num\">9.7&nbsp;&nbsp;</span>Creación de una función</a></span></li><li><span><a href=\"#Convertimos-a-un-DataFrame:\" data-toc-modified-id=\"Convertimos-a-un-DataFrame:-9.8\"><span class=\"toc-item-num\">9.8&nbsp;&nbsp;</span>Convertimos a un DataFrame:</a></span></li></ul></li><li><span><a href=\"#Extracción-de-datos-del-IBEX\" data-toc-modified-id=\"Extracción-de-datos-del-IBEX-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Extracción de datos del IBEX</a></span><ul class=\"toc-item\"><li><span><a href=\"#Petición-y-guardado-del-código-HTML:\" data-toc-modified-id=\"Petición-y-guardado-del-código-HTML:-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Petición y guardado del código HTML:</a></span></li><li><span><a href=\"#Buscamos-las-tablas-dentro-de-nuestra-página-web-y-extraemos-sus-datos:\" data-toc-modified-id=\"Buscamos-las-tablas-dentro-de-nuestra-página-web-y-extraemos-sus-datos:-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Buscamos las tablas dentro de nuestra página web y extraemos sus datos:</a></span></li><li><span><a href=\"#Creamos-una-función:\" data-toc-modified-id=\"Creamos-una-función:-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Creamos una función:</a></span></li><li><span><a href=\"#Convertimos-a-un-DataFrame:\" data-toc-modified-id=\"Convertimos-a-un-DataFrame:-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Convertimos a un DataFrame:</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a de Web Scraping\n",
    "## ¿Qué es el **Web Scraping**?\n",
    "\n",
    "El *web scraping* es una técnica esencial para extraer información de sitios web de manera automatizada. Nos permite acceder a datos que están disponibles en páginas web y utilizarlos para generar nuevas bases de datos o enriquecerlas con nuevos datos.\n",
    "\n",
    "Nuestro objetivo con el *web scraping* es obtener datos específicos de manera eficiente y automática, evitando la extracción manual que puede ser tediosa y propensa a errores. Esta técnica nos proporciona la capacidad de recopilar grandes cantidades de datos de manera rápida y sistemática.\n",
    "\n",
    "En Python, contamos con varias bibliotecas ampliamente utilizadas para realizar *web scraping*, cada una con sus propias características y fortalezas. Estas herramientas nos permiten analizar el código HTML o XML de una página web y extraer los datos necesarios de manera organizada:\n",
    "\n",
    "1. **Beautiful Soup**: Una biblioteca popular que facilita el análisis y la extracción de datos de páginas web. Nos permite buscar elementos utilizando etiquetas, atributos y contenido, simplificando la extracción de datos específicos.\n",
    "\n",
    "2. **Selenium**: Lo utilizamos para automatizar el navegador web, es útil cuando el contenido deseado se genera dinámicamente a través de JavaScript u otras interacciones del lado del cliente. Ofrece funciones para interactuar con elementos de la página, enviar formularios y más.\n",
    "\n",
    "3. **Scrapy**: Un framework de scraping web de alto nivel que proporciona una arquitectura completa y flexible para tareas de scraping eficientes a gran escala. Ofrece funcionalidades avanzadas como gestión de sesiones, almacenamiento de datos y manejo de errores.\n",
    "\n",
    "4. **lxml**: Una biblioteca eficiente para el procesamiento de XML y HTML en Python. Nos permite analizar y extraer datos de documentos XML/HTML utilizando XPath y selectores CSS.\n",
    "\n",
    "5. **PyQuery**: Inspirada en jQuery, nos permite utilizar expresiones jQuery para analizar y manipular documentos HTML/XML, facilitando la selección y extracción de elementos específicos.\n",
    "\n",
    "6. **Playwright**: Una biblioteca moderna para automatizar navegadores web, que ofrece soporte para múltiples lenguajes de programación entre ellos Python. Permite emular interacciones humanas con páginas web, como hacer click en botones, completar formularios y navegar por páginas, lo que resulta útil para tareas de scraping más avanzadas.\n",
    "\n",
    "Estas son solo algunas de las bibliotecas más populares para realizar *web scraping* en Python, cada una con sus propias características y fortalezas. La elección de la biblioteca dependerá de nuestros requisitos y la complejidad de la tarea de *scraping*.\n",
    "\n",
    "A lo largo del curso, nos centraremos en explorar detalladamente las tres primeras bibliotecas mencionadas: Beautiful Soup, Selenium y Scrapy. Estas herramientas son fundamentales para comprender los conceptos básicos del web scraping y desarrollar habilidades sólidas en la extracción automatizada de datos.Estas tres herramientas nos proporcionarán una base sólida para abordar una amplia gama de proyectos de web scraping y nos prepararán para enfrentar desafíos más complejos en la extracción de datos automatizada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a HTML\n",
    "## ¿Qué es **HTML**?\n",
    "\n",
    "**HTML**, acrónimo de *Hypertext Markup Language* (Lenguaje de Marcado de Hipertexto), es el lenguaje que se utiliza para definir la estructura y presentación de las páginas web.  A diferencia de los lenguajes de programación tradicionales, que se centran en la lógica y el comportamiento del programa, un lenguaje marcado se enfoca en etiquetar y estructurar partes del texto o del contenido de una página web, como imágenes, enlaces, formularios y otros elementos interactivos para indicar cómo deben ser interpretadas o presentadas por otro software, como los navegadores web.\n",
    "\n",
    "**HTML** se basa en una estructura jerárquica de etiquetas, también conocidas como elementos, que definen la estructura y el significado del contenido en una página web. Cada etiqueta tiene un propósito específico y se integra dentro del código HTML para dar formato y proporcionar información adicional sobre los elementos de la página.\n",
    "\n",
    "A continuación, tenemos un ejemplo básico de código HTML:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Título de la página</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Título principal</h1>\n",
    "    <p>Este es un párrafo de ejemplo.</p>\n",
    "    <a href=\"https://www.ejemplo.com\">Enlace a Ejemplo.com</a>\n",
    "    <img src=\"imagen.jpg\" alt=\"Descripción de la imagen\">\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Aquí tenemos una lista de algunas de las etiquetas HTML más comunes:\n",
    "\n",
    "- `<!DOCTYPE html>`: Declaramos que nuestro documento es un archivo HTML.\n",
    "- `<html>`: Es el elemento raíz que envuelve todo el contenido de la página.\n",
    "- `<head>`: Contiene información meta y enlaces a archivos externos, como hojas de estilo CSS o scripts de JavaScript.\n",
    "- `<title>`: Define el título de la página, que se muestra en la pestaña del navegador.\n",
    "- `<body>`: Contiene el contenido visible de la página web.\n",
    "- `<h1>`: Define un encabezado de nivel 1.\n",
    "- `<div>`: Define una sección genérica o un contenedor en un documento HTML.\n",
    "- `<p>`: Define un párrafo de texto.\n",
    "- `<a>`: Define un enlace a otra página web.\n",
    "- `<ul>`: Define una lista desordenada.\n",
    "- `<li>`: Define un elemento de lista dentro de una lista (`<ul>`).\n",
    "- `<img>`: Inserta una imagen en la página web.\n",
    "- `<table>`: Se utiliza para crear una tabla en HTML y envuelve todas las filas y columnas de la tabla.\n",
    "- `<tr>`: Se utilizan para definir filas dentro de una tabla `<table>`.\n",
    "- `<td>`: Representa una celda de datos en una tabla. Los elementos `<td>` deben estar ubicados dentro de un elemento `<tr>` para crear una celda en una fila.\n",
    "- `<th>`: Representa una celda de encabezado en una tabla. Al igual que `<td>`, los elementos `<th>` también deben estar ubicados dentro de un elemento `<tr>`, pero se utilizan para resaltar los encabezados de columna o fila.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selectores CSS\n",
    "## ¿ Qué son los selectores CSS?\n",
    "Los selectores CSS son patrones utilizados para seleccionar y estilizar elementos HTML en una página web. Permiten especificar qué elementos deben ser afectados por las reglas de estilo CSS. Los selectores CSS pueden apuntar a elementos específicos basados en su tipo, clase, ID, atributos u otras características.\n",
    "\n",
    "Aquí hay algunos ejemplos comunes de selectores CSS:\n",
    "\n",
    "- Selector de tipo: Selecciona todos los elementos de un tipo específico. Por ejemplo, `<p>` selecciona todos los párrafos.\n",
    "\n",
    "- Selector de clase: Selecciona elementos que tienen un atributo de clase específico. Por ejemplo, `.clase` selecciona todos los elementos con la clase \"clase\".\n",
    "\n",
    "- Selector de ID: Selecciona un elemento con un ID específico. Por ejemplo, `#id` selecciona el elemento con el ID \"id\".\n",
    "\n",
    "- Selector de atributo: Selecciona elementos que tienen un atributo específico. Por ejemplo, `[atributo]` selecciona elementos con el atributo \"atributo\".\n",
    "\n",
    "- Selector descendente: Selecciona elementos que son descendientes de otro elemento. Por ejemplo, `div p` selecciona todos los párrafos que están dentro de elementos `div`.\n",
    "\n",
    "- Selector hijo directo: Selecciona elementos que son hijos directos de otro elemento. Por ejemplo, `div > p` selecciona todos los párrafos que son hijos directos de elementos `div`.\n",
    "\n",
    "Los selectores CSS son ampliamente utilizados en el desarrollo web para aplicar estilos a elementos específicos de una página. En nuestro caso los utilizaremos   para seleccionar elementos mediante técnicas de web scraping para acceder a la información que deseamos extraer de una página web.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Beautiful Soup\n",
    "## ¿Qué es  **Beautiful Soup**?\n",
    "\n",
    "*Beautiful Soup* es una biblioteca de Python que nos permite la extracción de datos de archivos HTML y XML. Es una herramienta que simplifica  el proceso de navegación y manipulación de la estructura de estos documentos.\n",
    "Algunas características y ventajas de *Beautiful Soup* son:\n",
    "\n",
    "1. **Análisis de documentos HTML/XML**: Nos permite realizar el análisis de documentos HTML/XML y extraer datos específicos de manera eficiente. Nos ofrece métodos y funciones que facilitan el acceso a elementos, atributos y contenido dentro de la estructura del documento.\n",
    "\n",
    "2. **Interfaz intuitiva**: La interfaz de *Beautiful Soup* es fácil de entender y utilizar. No se requieren conocimientos avanzados de lenguajes  como HTML o XML. \n",
    "\n",
    "3. **Flexibilidad en analizadores**: Es compatible con varios analizadores HTML/XML, como `html.parser`, `lxml`, `html5lib`, entre otros. Esto nos permite seleccionar el analizador más adecuado según nuestras necesidades y la complejidad del documento que estamos analizando.\n",
    "\n",
    "4. **Navegación basada en árboles**: Utiliza una estructura de árbol para representar la jerarquía de los elementos en el documento HTML/XML. Esto nos permite realizar búsquedas y selecciones basadas en etiquetas, atributos, contenido y estructura del documento.\n",
    "\n",
    "5. **Extracción de datos personalizada**: Con *Beautiful Soup*, podemos filtrar y extraer datos específicos de una página web. Podemos buscar elementos por su nombre de etiqueta, atributos, clases o contenido. \n",
    "\n",
    "6. **Amplia adopción**: Es una de las herramientas más utilizadas y populares para realizar *web scraping* y análisis de datos en Python. Cuenta con una comunidad activa que proporciona soporte y continua mejorando la herramienta.\n",
    "\n",
    "Para empezar a usar esta biblioteca es necesario instalar *Beautiful Soup* . Para hacerlo tendremos que ejecutar el siguiente comando:\n",
    "\n",
    "```\n",
    "!pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para instalar beautifulsoup descomenta la siguiente linea de código:\n",
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos importantes de *Beautiful Soup*\n",
    "\n",
    "Algunos de los métodos principales de Beautiful Soup:\n",
    "\n",
    "1. **BeautifulSoup**: Es el constructor principal de Beautiful Soup. Utilizamos este constructor para crear un objeto BeautifulSoup que toma el código HTML o XML como entrada y lo analiza para crear dicho objeto. Un objeto BeautifulSoup es una estructura de datos en Python que representa el análisis de un documento HTML o XML  y que facilita la extracción de datos específicos de este documento. \n",
    "\n",
    "La sintaxis básica es:\n",
    "   ```python\n",
    "   # Importamos la librería\n",
    "   from bs4 import BeautifulSoup\n",
    "\n",
    "   # Creamos un objeto BeautifulSoup\n",
    "   soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "   ```\n",
    "\n",
    "2. **find() y findAll()**: Estos métodos son fundamentales para buscar elementos en un documento analizado con Beautiful Soup.\n",
    "   - `find()` **devuelve el primer elemento** que coincide con los criterios de búsqueda.\n",
    "\n",
    "   - `findAll()` **devuelve una lista de todos los elementos** que coinciden con los criterios de búsqueda.\n",
    "   \n",
    "      ```python\n",
    "      # Buscamos un elemento en el código HTML por su etiqueta\n",
    "      primer_elem_a = soup.find('a')\n",
    "\n",
    "      # Buscamos todos los elementos <a> en el código HTML y los almacenamos en una lista\n",
    "      lista_elem_a = soup.findAll('a')\n",
    "      ```\n",
    "\n",
    "3.  **select()**: Devuelve una **lista** de objetos *Tag*. Los objetos Tag representan las etiquetas HTML o XML que coinciden con el selector CSS especificado. Los objetos Tag son esenciales en BeautifulSoup, ya que representan las distintas partes del documento analizado, como elementos, atributos y contenido. Nuestra sintaxis básica es: \n",
    "      ```python\n",
    "      # Buscamos todos los elementos <a> dentro de elementos <li>\n",
    "      enlaces = soup.select('li a')\n",
    "      ```\n",
    "\n",
    "4. **text** o **getText()**: El atributo `text` devuelve el texto contenido dentro de una etiqueta. Ambos métodos hacen exactamente lo mismo. Accedemos a él de la siguiente manera:\n",
    "      ```python\n",
    "      # Obtenemos el texto de un elemento\n",
    "      elemento.text\n",
    "\n",
    "      # Obtenemos el texto de un elemento de la segunda forma\n",
    "      elemento.getText()\n",
    "      ```\n",
    "\n",
    "5. **get()**: El método `get()` se utiliza para obtener el valor de un atributo de una etiqueta. Aquí tenemos un ejemplo:\n",
    "      ```python\n",
    "      # Obtenemos el valor href de un atributo\n",
    "      elemento.get('href')\n",
    "      ```\n",
    "\n",
    "6. **prettify()** : El método `prettify()` se utiliza para imprimir o mostrar de manera más legible la estructura HTML o XML parseada por el objeto BeautifulSoup. Este método agrega sangrías y saltos de línea al documento HTML o XML, lo que facilita su lectura y comprensión al resaltar la estructura jerárquica del documento.La sintaxis básica para utilizar prettify() es la siguiente:\n",
    "      ```python\n",
    "      # Imprimir el documento HTML/XML de manera legible\n",
    "      print(soup.prettify())\n",
    "      ```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biblioteca requests\n",
    "\n",
    "Para comenzar con la extracción de datos usando *Web Scraping* necesitamos realizar una solicitud HTTP utilizando la biblioteca `requests` para hacer una petición al servidor y este nos devolverá una respuesta. Esta respuesta contiene información sobre el resultado de la solicitud.\n",
    "Una de las propiedades más comunes de esa respuesta es `status_code`, que es un código numérico que indica el estado de la respuesta. Recordemos que, al usar la librería `requests`, en realidad lo que estamos haciendo es pedir información a un servidor (en este caso, una página web). El método `status_code` nos indica si hemos pedido los datos correctamente o no. Algunos de los códigos más comunes son:\n",
    "\n",
    "- **200**: Esta respuesta indica que la solicitud fue exitosa. El servidor ha respondido correctamente y ha devuelto los datos solicitados. Esto es generalmente lo que esperamos obtener cuando hacemos una solicitud HTTP exitosa.\n",
    "\n",
    "- **404**: Este código indica que el recurso solicitado no se encontró en el servidor. Puede ocurrir cuando se accede a una URL incorrecta o cuando el recurso ha sido eliminado o no está disponible.\n",
    "\n",
    "- **500**: Un código de estado 500 indica un error interno del servidor. Esto puede deberse a un problema en el servidor que impide que se procese correctamente la solicitud. Es posible que necesitemos comunicarnos con el administrador del servidor en este caso.\n",
    "\n",
    "- **302**: Este código es una redirección temporal. Indica que la URL solicitada ha sido redirigida a otra ubicación.\n",
    "\n",
    "- **401**: Este código indica que la solicitud requiere autenticación. Significa que no tenemos acceso a los recursos solicitados sin proporcionar credenciales válidas, como un nombre de usuario y una contraseña.\n",
    "\n",
    "- **403**: Este código indica que la solicitud fue entendida por el servidor, pero este se niega a cumplirla. Esto puede deberse a restricciones de acceso en el servidor. Para intentar sortearlo, podemos cambiar el `User-Agent` en el `header` de la `request` como veremos más adelante.\n",
    "\n",
    "Usamos el método `status_code` en `requests`, para verificar el código de estado de la respuesta para determinar si nuestra solicitud fue exitosa o si hubo algún tipo de error. Si la solicitud fue exitosa **el código de estado será 200** . Si el código de estado es diferente a 200 tendremos que manejar el error o condición especial según el tipo de respuesta que recibamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías que necesitamos\n",
    "\n",
    "# Librerías de extracción de datos\n",
    "# -----------------------------------------------------------------------\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Tratamiento de datos\n",
    "# -----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Importación de las clases creadas en nuestro archivo de soporte\n",
    "# -----------------------------------------------------------------------\n",
    "from src import soporte_beautifullsoup as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de datos de wikipedia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer ejemplo que vamos a ver es como realizar la extracción de datos de la página de inicio de wikipedia mediante BeutifulSoup. \n",
    "```python\n",
    "url_wiki = \"https://es.wikipedia.org/wiki/Wikipedia:Portada\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta de la petición es: 200\n"
     ]
    }
   ],
   "source": [
    "# Definimos una variable con la url que de la que vamos a extraer la información\n",
    "url_wiki = \"https://es.wikipedia.org/wiki/Wikipedia:Portada\"\n",
    "\n",
    "# Definimos una variable con la petición usando la biblioteca requests\n",
    "res_wiki = requests.get(url_wiki)\n",
    "\n",
    "# Comprobamos el estado de la petición\n",
    "print(f\"La respuesta de la petición es: {res_wiki.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora que hemos comprobado que la petición es la adecuada empezamos a usar BeautifulSoup\n",
    "# Creamos un objeto BeautifullSoup\n",
    "sopa_wiki = BeautifulSoup(res_wiki.content, \"html.parser\")\n",
    "\n",
    "# Mostramos el contenido del objeto BeautifullSoup que acabamos de crear\n",
    "## print(sopa_wiki.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la ruta completa donde deseamos guardar el archivo HTML\n",
    "ruta_guardado = \"datos/WikipediaPortada.html\"\n",
    "\n",
    "# Guardamos el código HTML en la ubicación especificada\n",
    "html_wiki = sopa_wiki.prettify()\n",
    "with open(ruta_guardado, \"w\", encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(html_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que vamos a hacer es buscar las etiquetas principales que hemos visto antes:\n",
    "- `<head>`\n",
    "- `<title>`\n",
    "- `<body>`\n",
    "- `<h1>`\n",
    "- `<p>`\n",
    "- `<a>`\n",
    "- `<img>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista etiquetas_head es: 1\n"
     ]
    }
   ],
   "source": [
    "# Empezamos con la etiqueta head. Buscamos todas las etiquetas head con findAll\n",
    "etiquetas_head = sopa_wiki.findAll(\"head\")\n",
    "\n",
    "# Comprobamos la longitud de la lista\n",
    "print(f\"La longitud de la lista etiquetas_head es: {len(etiquetas_head)}\") \n",
    "\n",
    "# Si solo hay una etiqueta podemos usar el find y getText() para ver su contenido:\n",
    "etiqueta_head = sopa_wiki.find(\"head\").getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista etiquetas_title es: 1\n",
      "El texto en la etiqueta_title es: Wikipedia, la enciclopedia libre\n"
     ]
    }
   ],
   "source": [
    "# Ahora buscamos la etiqueta title\n",
    "etiquetas_title = sopa_wiki.findAll(\"title\")\n",
    "\n",
    "# Comprobamos la longitud de la lista\n",
    "print(f\"La longitud de la lista etiquetas_title es: {len(etiquetas_title)}\") \n",
    "\n",
    "# Si solo hay una podemos usar el find y getText() para ver su contenido\n",
    "etiqueta_title = sopa_wiki.find(\"title\").getText()\n",
    "print(f\"El texto en la etiqueta_title es: {etiqueta_title}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista etiquetas_body es: 1\n"
     ]
    }
   ],
   "source": [
    "# Vamos a buscar la etiqueta body:\n",
    "etiquetas_body = sopa_wiki.findAll(\"body\")\n",
    "\n",
    "# Comprobamos la longitud de la lista\n",
    "print(f\"La longitud de la lista etiquetas_body es: {len(etiquetas_body)}\") \n",
    "\n",
    "\n",
    "# Si solo hay una podemos usar el find y getText() para ver su contenido\n",
    "etiqueta_body = sopa_wiki.find(\"body\").getText()\n",
    "## print(f\"El texto en la etiquetas_body es: {etiquetas_body}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista etiquetas_h1 es: 2\n",
      "El texto en la 0 etiqueta es: Wikipedia:Portada\n",
      "El texto en la 1 etiqueta es: Bienvenidos a Wikipedia,\n"
     ]
    }
   ],
   "source": [
    "# Continuamos con la etiqueta h1:\n",
    "etiquetas_h1 = sopa_wiki.findAll(\"h1\")\n",
    "\n",
    "# Comprobamos la longitud de la lista\n",
    "print(f\"La longitud de la lista etiquetas_h1 es: {len(etiquetas_h1)}\") \n",
    "\n",
    "# Tenemos más de una etiqueta h1, vamos a ver el texto contenido en cada h1:\n",
    "for i, etiqueta in enumerate(etiquetas_h1):\n",
    "    print(f\"El texto en la {i} etiqueta es: {etiqueta.getText()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista etiquetas_a es: 367\n",
      "El texto en la 0 etiqueta es: Ir al contenido\n",
      "El texto en la 1 etiqueta es: Portada\n",
      "El texto en la 2 etiqueta es: Portal de la comunidad\n",
      "El texto en la 3 etiqueta es: Actualidad\n",
      "El texto en la 4 etiqueta es: Cambios recientes\n"
     ]
    }
   ],
   "source": [
    "# Vamos con las etiquetas a, para ver los enlaces de la página:\n",
    "etiquetas_a = sopa_wiki.findAll(\"a\")\n",
    "\n",
    "# Comprobamos la longitud de la lista\n",
    "print(f\"La longitud de la lista etiquetas_a es: {len(etiquetas_a)}\") \n",
    "\n",
    "# Tenemos un número considerable de etiquetas a en la página, vamos a ver el texto contenido en cada una de ellas:\n",
    "for i, etiqueta in enumerate(etiquetas_a[:5]):\n",
    "    print(f\"El texto en la {i} etiqueta es: {etiqueta.getText()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista etiquetas_img es: 34\n",
      "La ruta de la imagen en la 0 etiqueta es: /static/images/icons/wikipedia.png\n",
      "La ruta de la imagen en la 1 etiqueta es: /static/images/mobile/copyright/wikipedia-wordmark-en.svg\n",
      "La ruta de la imagen en la 2 etiqueta es: /static/images/mobile/copyright/wikipedia-tagline-es.svg\n",
      "La ruta de la imagen en la 3 etiqueta es: //upload.wikimedia.org/wikipedia/commons/thumb/8/86/Madonna-Like_A_Virgin-CD.png/220px-Madonna-Like_A_Virgin-CD.png\n",
      "La ruta de la imagen en la 4 etiqueta es: //upload.wikimedia.org/wikipedia/commons/thumb/b/b3/OOjs_UI_icon_ellipsis.svg/20px-OOjs_UI_icon_ellipsis.svg.png\n"
     ]
    }
   ],
   "source": [
    "# Ahora vamos a buscar las imágenes:\n",
    "etiquetas_img = sopa_wiki.findAll(\"img\")\n",
    "\n",
    "# Comprobamos la longitud de la lista\n",
    "print(f\"La longitud de la lista etiquetas_img es: {len(etiquetas_img)}\") \n",
    "\n",
    "# Tenemos un numero considerable de etiquetas a en la página. Vamos a ver al contenido de una manera diferente\n",
    "# esta vez usaremos get(\"src\"), el atributo src hace referencia a la ruta para acceder a las imagenes de una página web.\n",
    "for i, etiqueta in enumerate(etiquetas_img[:5]):\n",
    "    print(f\"La ruta de la imagen en la {i} etiqueta es: {etiqueta.get('src')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de una función:\n",
    "Ahora vamos a crear una función que recoga todos los pasos que hemos ido siguiendo para incluirla en nuestra carpeta \"src\" y poder usarla para extraer los datos que queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta de la petición es: 200\n",
      "El título de la portada de Wikipedia es: ['Wikipedia, la enciclopedia libre']\n"
     ]
    }
   ],
   "source": [
    "# LLamamos a la función que acabamos de crear \n",
    "extraccion_wiki = sb.wikipedia_portada(url_wiki)\n",
    "\n",
    "# Comprobamos que funciona mostrando algunas de las claves del diccionario que devuelve\n",
    "print(f\"El título de la portada de Wikipedia es: {extraccion_wiki['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de datos de zapatillas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a extraer datos de zapatillas de la página web de Ebay y almacenarlos en un csv. El objetivo es conseguir un *dataframe* que recoja la descripción de la zapatilla que aporta el vendedor, el precio , los gastos de envío, el tipo de vendedor(una empresa o un particular) y la url de la imagen del producto.\n",
    "```python\n",
    "url_zapatillas = \"https://www.ebay.es/e/_moda/adidassneakerses?_pgn=1\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Petición y guardado del código HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta de la petición es: 200\n"
     ]
    }
   ],
   "source": [
    "# Definimos una variable con la url que de la que vamos a extraer la información\n",
    "url_zapatillas = \"https://www.ebay.es/e/_moda/adidassneakerses?_pgn=1\"\n",
    "\n",
    "# Definimos una variable con la petición\n",
    "res_zapatillas = requests.get(url_zapatillas)\n",
    "\n",
    "# Comprobamos el estado de la petición\n",
    "print(f\"La respuesta de la petición es: {res_zapatillas.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto BeautifulSoup para poder acceder al contenido solicitado\n",
    "sopa_zapatillas = BeautifulSoup(res_zapatillas.content, 'html.parser')\n",
    "\n",
    "# Mostramos el contenido del objeto BeautifullSoup que acabamos de crear\n",
    "## (printsopa_zapatillas.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la ruta completa donde deseamos guardar el archivo HTML\n",
    "ruta_guardado = \"datos/EbayZapatillas.html\"\n",
    "\n",
    "# Guardamos el código HTML en la ubicación especificada\n",
    "html_zapatillas = sopa_zapatillas.prettify()\n",
    "with open(ruta_guardado, \"w\", encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(html_zapatillas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del producto\n",
    "Lo primero que vamos a hacer es extraer la descripción de todas las zapatillas que tenemos en la página web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista es: 60\n",
      "La descripción de la 1 zapatilla es: Adidas Zapatos de Fútbol PREDATOR LEAGUE TF Azul Lúcido/Calzado Blanco ID0910 ¡NUEVO!\n",
      "La descripción de la 2 zapatilla es: adidas Crazy 1 2024 Playoffs Kobe Bryant Men Basketball Hoopers Shoes IE6570\n",
      "La descripción de la 3 zapatilla es: Adidas Attitude Ivory Team College Borgoña IF6280 Hombres Talla\n",
      "La descripción de la 4 zapatilla es: Zapatillas para hombre adidas Originals Forum 84 Low x M&M NUEVAS\n",
      "La descripción de la 5 zapatilla es: [EF4366] Adidas ZX 8000 para hombre\n"
     ]
    }
   ],
   "source": [
    "# Extraemos la descripción de los zapatillas con el método findAll\n",
    "lista_descripcion_producto = sopa_zapatillas.findAll(\"h3\", {\"class\" :\"s-item__title\"})\n",
    "\n",
    "# Comprobamos los resultados de la lista que acabamos de crear\n",
    "print(f\"La longitud de la lista es: {len(lista_descripcion_producto)}\")\n",
    "for i, zapatilla in enumerate(lista_descripcion_producto[:5]):\n",
    "    print(f\"La descripción de la {i + 1} zapatilla es: {zapatilla.getText()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora vamos a crear una lista donde iremos almacenado la descripción de las zapatillas\n",
    "descripcion_productos = []\n",
    "\n",
    "# Como tenemos lista_descripcion_producto lo que vamos a hacer es iterar por la lista para poder acceder a cada uno de los elementos\n",
    "for zapatilla in lista_descripcion_producto:\n",
    "    # Utilizamos el método \".getText()\" para sacar el texto de cada uno de los elementos y lo añadimos a la lista que hemos creado previamente. \n",
    "    descripcion_productos.append(zapatilla.getText())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precio producto\n",
    "Seguimos extrayendo más información, vamos a extraer los precios de cada zapatilla. El objetivo es tener una lista con los precios en formato numérico: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista de precios es: 60\n",
      "El precio de la 1 zapatilla es: 106,94 EUR\n",
      "El precio de la 2 zapatilla es: 160,27 EUR\n",
      "El precio de la 3 zapatilla es: 174,14 EUR\n",
      "El precio de la 4 zapatilla es: 100,00 EUR\n",
      "El precio de la 5 zapatilla es: 111,65 EUR\n"
     ]
    }
   ],
   "source": [
    "# Extraemos los precio de los zapatillas con el método findAll\n",
    "lista_precios = sopa_zapatillas.findAll(\"span\", {\"class\" : \"s-item__price\"})\n",
    "\n",
    "# Comprobamos los resultados de la lista que acabamos de crear\n",
    "print(f\"La longitud de la lista de precios es: {len(lista_precios)}\")\n",
    "\n",
    "for i, precio in enumerate(lista_precios[:5]):\n",
    "    print(f\"El precio de la {i + 1} zapatilla es: {precio.getText()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de precios\n",
    "Ya tenemos los precios de todas las zapatillas, pero vemos que estos datos están separados por una coma en vez de un punto y tienen incluido EUR, por lo que están en formato *string*. Queremos extraer los precios directamente como números por lo que:\n",
    "\n",
    "- Eliminamos EUR del final.\n",
    "\n",
    "- Reemplazamos las \",\" por \".\", ya que en Python los números decimales tienen que ir con \".\"\n",
    "\n",
    "- Tenemos algunas zapatillas con dos precios, nos quedamos solo con el primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los resultados después de limpiar los precios son :\n",
      " [106.94, 160.27, 174.14, 100.0, 111.65, 156.49, 100.0, 175.6, 120.42, 180.92, 178.17, 100.0, 106.0, 245.99, 172.99, 119.9, 165.08, 167.0, 139.0, 172.7, 169.99, 165.0, 165.83, 158.09, 121.61, 175.6, 119.0, 159.9, 159.9, 164.58, 129.95, 159.9, 158.09, 235.46, 119.0, 116.16, 163.16, 102.85, 139.95, 100.0, 163.96, 161.28, 137.25, 143.72, 119.9, 198.77, 155.1, 101.17, 173.47, 111.1, 124.95, 137.98, 174.01, 239.57, 171.78, 143.72, 155.33, 100.0, 121.61, 130.68]\n"
     ]
    }
   ],
   "source": [
    "# Creamos una list comprehension para realizar los cambios en los datos que acabamos de extraer:\n",
    "precio_productos = [float(precio.getText().split()[0].replace(\",\", \".\")) for precio in lista_precios]\n",
    "\n",
    "# Comprobamos que se han limpiado los datos\n",
    "print(f\"Los resultados después de limpiar los precios son :\\n {precio_productos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gastos de envío\n",
    "Ahora vamos a extraer el precio de los gastos de envío. Al igual que en el paso anterior, el objetivo es tener una lista con los precios en formato numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista de gastos de envío es: 60\n",
      "El precio de los gastos de envío de la 1 zapatilla es: Envío gratis\n",
      "El precio de los gastos de envío de la 2 zapatilla es: 6,16 EUR de envío\n",
      "El precio de los gastos de envío de la 3 zapatilla es: 5,46 EUR de envío\n",
      "El precio de los gastos de envío de la 4 zapatilla es: 10,00 EUR de envío\n",
      "El precio de los gastos de envío de la 5 zapatilla es: 34,22 EUR de envío\n"
     ]
    }
   ],
   "source": [
    "# Extraemos los precio antiguos de los zapatillas con el método findAll\n",
    "lista_gastos_envio = sopa_zapatillas.findAll(\"span\", {\"class\" : \"s-item__shipping s-item__logisticsCost\"})\n",
    "\n",
    "# Comprobamos los resultados de la lista que acabamos de crear\n",
    "print(f\"La longitud de la lista de gastos de envío es: {len(lista_gastos_envio)}\")\n",
    "\n",
    "for i, precio in enumerate(lista_gastos_envio[:5]):\n",
    "    print(f\"El precio de los gastos de envío de la {i + 1} zapatilla es: {precio.getText()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de gastos de envío\n",
    "Tenemos el mismo problema que en el caso anterior, los precios se extraen como un *string* y no como un *float*. Tenemos que limpiarlos antes de guardarlos en nuestra lista definitiva de gastos_envio:\n",
    "- Eliminamos EUR del final.\n",
    "\n",
    "- Reemplazamos las \",\" por \".\", ya que en Python los números decimales tienen que ir con \".\"\n",
    "\n",
    "- Si tenemos envío gratis lo guardamos como 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La información contenida en la lista 'gastos_envio' es:\n",
      " [0, 6.16, 5.46, 10.0, 34.22, 0, 13.99, 0, 32.11, 4.49, 42.64, 10.0, 9.9, 0, 0, 20.0, 0, 0, 5.0, 0, 0, 0, 38.27, 22.1, 47.83, 0, 5.0, 9.14, 9.14, 6.16, 15.0, 9.14, 49.08, 6.16, 5.0, 0, 0, 0, 10.0, 20.0, 6.16, 17.9, 8.42, 11.06, 20.0, 4.49, 0, 10.84, 21.65, 39.49, 10.0, 0, 0, 4.49, 0, 32.21, 0, 13.99, 11.06, 0]\n"
     ]
    }
   ],
   "source": [
    "# Definimos una lista para guardar cada uno de los precios\n",
    "gastos_envio = []\n",
    "\n",
    "# Iteramos por cada elemento de la lista_gastos_envio\n",
    "for envio in lista_gastos_envio:\n",
    "    # Separamos por el espacio y nos quedamos con la primera parte\n",
    "    gasto = envio.getText().split()[0]\n",
    "    # Si gasto es igual a Envío significa que es gratis\n",
    "    if gasto == \"Envío\":\n",
    "        gastos_envio.append(0)\n",
    "    else:\n",
    "        gastos_envio.append(float(gasto.replace(\",\", \".\")))\n",
    "\n",
    "print(f\"La información contenida en la lista 'gastos_envio' es:\\n {gastos_envio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O con la list comprehension:\n",
    "gastos_envio = [0 if envio.getText().split()[0] == \"Envío\" else float(envio.getText().split()[0].replace(\",\", \".\"))for envio in lista_gastos_envio]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipo de vendedor:\n",
    "Ahora queremos extraer la información del tipo de vendedor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista de tipo de vendedor es: 60\n",
      "El tipo de vendedor de la 1 zapatilla es: Empresa\n",
      "El tipo de vendedor de la 2 zapatilla es: Empresa\n",
      "El tipo de vendedor de la 3 zapatilla es: Empresa\n",
      "El tipo de vendedor de la 4 zapatilla es: Empresa\n",
      "El tipo de vendedor de la 5 zapatilla es: Empresa\n"
     ]
    }
   ],
   "source": [
    "# Extraemos la categoría de las zapatillas con el método findAll\n",
    "lista_tipo_vendedor = sopa_zapatillas.findAll(\"div\", {\"class\" : \"s-item__subtitle\"})\n",
    "\n",
    "# Comprobamos los resultados de la lista que acabamos de crear\n",
    "print(f\"La longitud de la lista de tipo de vendedor es: {len(lista_tipo_vendedor)}\")\n",
    "\n",
    "for i, categoria in enumerate(lista_tipo_vendedor[:5]):\n",
    "    print(f\"El tipo de vendedor de la {i + 1} zapatilla es: {categoria.getText()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La información contenida en la lista 'tipo_vendedor' es:\n",
      " ['Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Particular', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Empresa', 'Particular', 'Empresa', 'Empresa', 'Empresa']\n"
     ]
    }
   ],
   "source": [
    "# Creamos una list comprehension para guardar los datos de las categorías\n",
    "tipo_vendedor = [categoria.getText() for categoria in lista_tipo_vendedor]\n",
    "print(f\"La información contenida en la lista 'tipo_vendedor' es:\\n {tipo_vendedor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagen del producto\n",
    "Por último, vamos a extraer la url para acceder la imagen de cada zapatilla. Recordamos que esta contenida en el código `HTML`dentro del atributo `src`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista de imagenes es: 60\n",
      "La imagen de la 0 zapatilla es: https://i.ebayimg.com/thumbs/images/g/Hv0AAOSw4zxmhnA~/s-l300.jpg\n",
      "La imagen de la 1 zapatilla es: https://i.ebayimg.com/thumbs/images/g/KcUAAOSwpmtnDLi6/s-l300.jpg\n",
      "La imagen de la 2 zapatilla es: https://i.ebayimg.com/thumbs/images/g/gLUAAOSwCbtml0vp/s-l300.jpg\n",
      "La imagen de la 3 zapatilla es: https://i.ebayimg.com/thumbs/images/g/rWMAAOSwKoBjUtnu/s-l300.jpg\n",
      "La imagen de la 4 zapatilla es: https://i.ebayimg.com/thumbs/images/g/AH8AAOSwWdhjZUSS/s-l300.jpg\n"
     ]
    }
   ],
   "source": [
    "# Extraemos las imágenes de las zapatillas con el método findAll\n",
    "lista_imagenes = sopa_zapatillas.findAll(\"img\", {\"class\": \"s-item__image-img\"})\n",
    "\n",
    "# Comprobamos los resultados de la lista que acabamos de crear\n",
    "print(f\"La longitud de la lista de imagenes es: {len(lista_imagenes)}\")\n",
    "\n",
    "for i, imagen in enumerate(lista_imagenes[:5]):\n",
    "    print(f\"La imagen de la {i} zapatilla es: {imagen.get('src')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una list comprehension para guardar los datos de las categorías\n",
    "imagenes_producto = [imagen.get(\"src\") for imagen in lista_imagenes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de una función\n",
    "Ahora vamos a crear una función que recoga todos los pasos que hemos ido siguiendo para incluirla en nuestra carpeta \"src\" y poder usarla para extraer los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta de la petición es: 200\n"
     ]
    }
   ],
   "source": [
    "# Extraemos los datos de la primera página:\n",
    "url = \"https://www.ebay.es/e/_moda/adidassneakerses?_pgn=1\"\n",
    "datos_ebay = sb.sacar_zapatillas(url)\n",
    "#print(f\"El resultado de nuestra extracción es: {datos_ebay}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a utilizar nuestra función para extraer la información de varias páginas de Ebay una detrás de otra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos una lista para almacenar todos los resultados de las páginas\n",
    "todos_resultados = []\n",
    "\n",
    "# Inicializamos un bucle que recorrerá 6 páginas de resultados en eBay\n",
    "for pagina in range(6):\n",
    "    # Construimos la URL de la página actual con el número de página\n",
    "    url_zapatillas_todas = f\"https://www.ebay.es/e/_moda/adidassneakerses?_pgn={pagina}\"\n",
    "    \n",
    "    # Llamamos a la función 'sacar_zapatillas' para extraer información de los móviles en la página actual\n",
    "    resultados_pagina_actual = sb.sacar_zapatillas(url_zapatillas_todas)\n",
    "    \n",
    "    # Añadimos los resultados de la página actual a la lista de todos los resultados\n",
    "    todos_resultados.append(resultados_pagina_actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n",
      "La respuesta de la petición es: 200\n"
     ]
    }
   ],
   "source": [
    "# O podemos hacerlo con list comprehension\n",
    "todos_resultados = [sb.sacar_zapatillas(f\"https://www.ebay.es/e/_moda/adidassneakerses?_pgn={pagina}\") for pagina in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertimos a un DataFrame:\n",
    "El siguiente paso que vamos a hacer es convertir las listas que nos devuelve la función sacar_zapatillas en un DataFrame usando la librería **Pandas** para poder trabajar con los datos que hemos sacado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la lista de diccionarios en DataFrame usando pandas \n",
    "df_zapatillas = pd.concat([pd.DataFrame(r) for r in todos_resultados])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descripción</th>\n",
       "      <th>precio</th>\n",
       "      <th>gastos_envio</th>\n",
       "      <th>tipo_vendedor</th>\n",
       "      <th>imagen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adidas Zapatos de Fútbol PREDATOR LEAGUE TF Az...</td>\n",
       "      <td>106.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Empresa</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/Hv0AAOSw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adidas Crazy 1 2024 Playoffs Kobe Bryant Men B...</td>\n",
       "      <td>160.27</td>\n",
       "      <td>6.16</td>\n",
       "      <td>Empresa</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/KcUAAOSw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adidas Attitude Ivory Team College Borgoña IF6...</td>\n",
       "      <td>174.14</td>\n",
       "      <td>5.46</td>\n",
       "      <td>Empresa</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/gLUAAOSw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zapatillas para hombre adidas Originals Forum ...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Empresa</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/rWMAAOSw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[EF4366] Adidas ZX 8000 para hombre</td>\n",
       "      <td>111.65</td>\n",
       "      <td>34.22</td>\n",
       "      <td>Empresa</td>\n",
       "      <td>https://i.ebayimg.com/thumbs/images/g/AH8AAOSw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         descripción  precio  gastos_envio  \\\n",
       "0  Adidas Zapatos de Fútbol PREDATOR LEAGUE TF Az...  106.94          0.00   \n",
       "1  adidas Crazy 1 2024 Playoffs Kobe Bryant Men B...  160.27          6.16   \n",
       "2  Adidas Attitude Ivory Team College Borgoña IF6...  174.14          5.46   \n",
       "3  Zapatillas para hombre adidas Originals Forum ...  100.00         10.00   \n",
       "4                [EF4366] Adidas ZX 8000 para hombre  111.65         34.22   \n",
       "\n",
       "  tipo_vendedor                                             imagen  \n",
       "0       Empresa  https://i.ebayimg.com/thumbs/images/g/Hv0AAOSw...  \n",
       "1       Empresa  https://i.ebayimg.com/thumbs/images/g/KcUAAOSw...  \n",
       "2       Empresa  https://i.ebayimg.com/thumbs/images/g/gLUAAOSw...  \n",
       "3       Empresa  https://i.ebayimg.com/thumbs/images/g/rWMAAOSw...  \n",
       "4       Empresa  https://i.ebayimg.com/thumbs/images/g/AH8AAOSw...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a ver por encima el dataframe que acabamos de crear:\n",
    "df_zapatillas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      " (359, 5) \n",
      "\n",
      "Información:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 359 entries, 0 to 59\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   descripción    359 non-null    object \n",
      " 1   precio         359 non-null    float64\n",
      " 2   gastos_envio   359 non-null    float64\n",
      " 3   tipo_vendedor  359 non-null    object \n",
      " 4   imagen         359 non-null    object \n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 16.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vamos a ver algunas caraccterísticas de nuestro dataframe:\n",
    "# Vemos el número de filas y de columnas:\n",
    "print(f\"Shape:\\n {df_zapatillas.shape} \\n\")\n",
    "\n",
    "\n",
    "# Vemos toda la iformación del dataframe\n",
    "print(f\"Información:\\n\")\n",
    "display(df_zapatillas.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precio</th>\n",
       "      <td>359.0</td>\n",
       "      <td>148.196379</td>\n",
       "      <td>42.684878</td>\n",
       "      <td>75.0</td>\n",
       "      <td>116.075</td>\n",
       "      <td>146.17</td>\n",
       "      <td>168.56</td>\n",
       "      <td>420.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gastos_envio</th>\n",
       "      <td>359.0</td>\n",
       "      <td>13.196546</td>\n",
       "      <td>15.629465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.16</td>\n",
       "      <td>21.13</td>\n",
       "      <td>53.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count        mean        std   min      25%     50%     75%  \\\n",
       "precio        359.0  148.196379  42.684878  75.0  116.075  146.17  168.56   \n",
       "gastos_envio  359.0   13.196546  15.629465   0.0    0.000    6.16   21.13   \n",
       "\n",
       "                 max  \n",
       "precio        420.00  \n",
       "gastos_envio   53.04  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos la información preliminar de las columnas numéricas\n",
    "df_zapatillas.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>descripción</th>\n",
       "      <td>359</td>\n",
       "      <td>283</td>\n",
       "      <td>adidas Crazy 1 2024 Playoffs Kobe Bryant Men B...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tipo_vendedor</th>\n",
       "      <td>359</td>\n",
       "      <td>2</td>\n",
       "      <td>Empresa</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imagen</th>\n",
       "      <td>359</td>\n",
       "      <td>41</td>\n",
       "      <td>https://ir.ebaystatic.com/cr/v/c1/s_1x2.gif</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count unique                                                top  \\\n",
       "descripción     359    283  adidas Crazy 1 2024 Playoffs Kobe Bryant Men B...   \n",
       "tipo_vendedor   359      2                                            Empresa   \n",
       "imagen          359     41        https://ir.ebaystatic.com/cr/v/c1/s_1x2.gif   \n",
       "\n",
       "              freq  \n",
       "descripción      6  \n",
       "tipo_vendedor  334  \n",
       "imagen         311  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos la información preliminar de todas las columnas\n",
    "df_zapatillas.describe(include = \"object\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de datos del IBEX\n",
    "En el siguiente ejemplo vamos a ver como extraer datos de una tabla de una página web. Como en los ejemplos anteriores, vamos a utilizar las bibliotecas como `requests` y `BeautifulSoup` para extraer la información y `Pandas` para construir un *dataframe*.\n",
    "\n",
    "En concreto vamos a extraer los datos de los precios del IBEX-35 de la siguiente página web:\n",
    "```python\n",
    "url_ibex = \"https://www.bolsamania.com/indice/IBEX-35/historico-precios\"\n",
    "```\n",
    "Vamos a recordar los principales elementos de una tabla en **HTML** vistos en esta lección:\n",
    "\n",
    "- `<table>`: Este elemento se utiliza para crear una tabla en HTML y envuelve todas las filas y columnas de la tabla.\n",
    "\n",
    "- `<tr>`: Se utilizan para definir filas dentro de una tabla. Estos elementos hacen referencia a las filas de la tabla y deben estar ubicados dentro del elemento `<table>`\n",
    "\n",
    "- `<td>`: Representa una celda de datos en una tabla. Los elementos `<td>` deben estar ubicados dentro de un elemento `<tr>` para crear una celda en una fila.\n",
    "\n",
    "- `<th>`: Representa una celda de encabezado en una tabla. Al igual que `<td>`, los elementos `<th>` también deben estar ubicados dentro de un elemento `<tr>`, pero se utilizan para resaltar los encabezados de columna o fila."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Petición y guardado del código HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta de la petición es: 200\n"
     ]
    }
   ],
   "source": [
    "# Definimos una variable con la url que de la que vamos a extraer la información\n",
    "url_ibex = \"https://www.bolsamania.com/indice/IBEX-35/historico-precios\"\n",
    "\n",
    "# Definimos una variable con la petición usando la biblioteca requests\n",
    "res_ibex = requests.get(url_ibex)\n",
    "\n",
    "# Comprobamos el estado de la petición\n",
    "print(f\"La respuesta de la petición es: {res_ibex.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un objeto BeautifullSoup\n",
    "sopa_ibex = BeautifulSoup(res_ibex.content, \"html.parser\")\n",
    "\n",
    "# Mostramos el contenido del objeto BeautifullSoup que acabamos de crear\n",
    "## print(sopa_ibex.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la ruta completa donde deseamos guardar el archivo HTML\n",
    "ruta_guardado = \"datos/Ibex35.html\"\n",
    "\n",
    "# Guardamos el código HTML en la ubicación especificada\n",
    "html_ibex = sopa_ibex.prettify()\n",
    "with open(ruta_guardado, \"w\", encoding=\"utf-8\") as archivo:\n",
    "    archivo.write(html_ibex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscamos las tablas dentro de nuestra página web y extraemos sus datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de tablas que tenemos en la página web es: 3\n"
     ]
    }
   ],
   "source": [
    "tablas = sopa_ibex.findAll(\"table\")\n",
    "\n",
    "print(f\"El número de tablas que tenemos en la página web es: {len(tablas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la tabla 1 tenemos: \n",
      " Periodo\n",
      "\n",
      "                            \n",
      "En la tabla 2 tenemos: \n",
      " Diferencia\n",
      "494,6000\n",
      "\n",
      "\n",
      "Promedio\n",
      "11.742\n",
      "En la tabla 3 tenemos: \n",
      " Fecha\n",
      "Precio\n",
      "Variación %\n",
      "Máximo\n",
      "Mínim\n"
     ]
    }
   ],
   "source": [
    "for i, tabla in enumerate(tablas):\n",
    "    print(f\"En la tabla {i+ 1} tenemos: \\n {tabla.getText()[3:40]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista de encabezados es: 6\n",
      "El encabezado 0 es: Fecha\n",
      "El encabezado 1 es: Precio\n",
      "El encabezado 2 es: Variación %\n",
      "El encabezado 3 es: Máximo\n",
      "El encabezado 4 es: Mínimo\n",
      "El encabezado 5 es: Apertura\n"
     ]
    }
   ],
   "source": [
    "# Guardamos en una variable los datos de la tabla que nos interesa, en nuestro caso la tercera\n",
    "nuestra_tabla = tablas[2]\n",
    "\n",
    "# Primero buscamos los encabezados de la tabla usando la etiqueta th y los contenemos en una lista usando findAll\n",
    "lista_encabezados = nuestra_tabla.findAll(\"th\")\n",
    "\n",
    "# Comprobamos los resultados de la lista que acabamos de crear\n",
    "print(f\"La longitud de la lista de encabezados es: {len(lista_encabezados)}\")\n",
    "\n",
    "for i, encabezado in enumerate(lista_encabezados):\n",
    "    print(f\"El encabezado {i} es: {encabezado.getText()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La longitud de la lista de filas es: 21\n",
      "La fila 1 es: \n",
      "Fecha\n",
      "Precio\n",
      "Variación %\n",
      "Máximo\n",
      "Mínimo\n",
      "Apertura\n",
      "\n",
      "La fila 2 es: \n",
      "16-sep-24\n",
      "11.581,000\n",
      "0,35%\n",
      "11.592,500\n",
      "11.510,000\n",
      "11.520,800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraemos todas las filas de la variable nuestra_tabla, usando el método findAll() con la etiqueta tr\n",
    "lista_filas_ibex = nuestra_tabla.findAll(\"tr\")\n",
    "\n",
    "# Comprobamos los resultados de la lista que acabamos de crear\n",
    "print(f\"La longitud de la lista de filas es: {len(lista_filas_ibex)}\")\n",
    "\n",
    "for i, fila in enumerate(lista_filas_ibex[:2]):\n",
    "    print(f\"La fila {i + 1} es: {fila.getText()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los resultados de iterar por la lista son:\n",
      " [['16-sep-24', '11.581,000', '0,35%', '11.592,500', '11.510,000', '11.520,800'], ['17-sep-24', '11.703,400', '1,06%', '11.753,900', '11.620,700', '11.620,900']]\n"
     ]
    }
   ],
   "source": [
    "# Creamos una lista llamada 'resultados_ibex' para almacenar los resultados obtenidos.\n",
    "resultados_ibex = []\n",
    "\n",
    "# Iniciamos un bucle 'for' para iterar a través de la lista_filas_ibex a partir de la segunda fila\n",
    "for fila in lista_filas_ibex[1:]:\n",
    "    # Para cada 'fila', extraemos el texto, lo dividimos en una lista usando '\\n' como separador \n",
    "    fila_texto = fila.text\n",
    "    #Eliminamos el primer y último elemento de la lista, ambos <tr>\n",
    "    elementos_fila = fila_texto.split(\"\\n\")[1:-1]\n",
    "\n",
    "    # Añadimos la lista de elementos a la lista 'resultados_ibex'.\n",
    "    resultados_ibex.append(elementos_fila)\n",
    "\n",
    "# Imprimimos los resultados obtenidos después de iterar por la lista.\n",
    "print(f\"Los resultados de iterar por la lista son:\\n {resultados_ibex[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los resultados después de limpiar los datos son: \n",
      " [['16-sep-24', 11581.0, 0.35, 11592.5, 11510.0, 11520.8], ['17-sep-24', 11703.4, 1.06, 11753.9, 11620.7, 11620.9]]\n"
     ]
    }
   ],
   "source": [
    "# Ahora vamos a limpiar los datos para guardarlos de forma correcta\n",
    "\n",
    "# Iteramos a través de cada lista en 'resultados_ibex' y sus elementos usando 'enumerate'.\n",
    "for indice_lista, lista in enumerate(resultados_ibex):\n",
    "    for indice_elemento, elemento in enumerate(lista):\n",
    "        try:\n",
    "            # Intentamos convertir cada elemento de la lista en un número de punto flotante.\n",
    "            # Para ello, eliminamos caracteres especiales como '%' y cambiamos los puntos por nada y las comas por puntos.\n",
    "            resultados_ibex[indice_lista][indice_elemento] = float(elemento.replace(\"%\", \"\").replace(\".\", \"\").replace(\",\", \".\"))\n",
    "        except:\n",
    "            # Si se produce una excepción (por ejemplo, si el elemento no es convertible a  punto flotante), \n",
    "            # mantenemos el elemento original en la lista.\n",
    "            resultados_ibex[indice_lista][indice_elemento] = elemento\n",
    "    \n",
    "# Imprimimos los resultados después de limpiar los datos.\n",
    "print(f\"Los resultados después de limpiar los datos son: \\n {resultados_ibex[:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos una función:\n",
    "Ahora vamos a crear la función que recoga todos los pasos que hemos ido siguiendo para incluirla en nuestra carpeta \"src\" como hemos hecho en los ejemplos anteriores para después poder extraer los datos con dicha función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La respuesta de la petición es: 200\n"
     ]
    }
   ],
   "source": [
    "# Llamamos a la función y guardamos los datos en dos nuevas variables\n",
    "encabezados_ibex_final, datos_ibex_final = sb.sacar_tabla_ibex(url_ibex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertimos a un DataFrame:\n",
    "El siguiente paso que vamos a hacer es convertir las listas que nos devuelve la función sacar_tabla_ibex en un DataFrame usando la librería **Pandas** para poder trabajar con los datos que hemos sacado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-sep-24</td>\n",
       "      <td>11581.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>11592.5</td>\n",
       "      <td>11510.0</td>\n",
       "      <td>11520.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17-sep-24</td>\n",
       "      <td>11703.4</td>\n",
       "      <td>1.06</td>\n",
       "      <td>11753.9</td>\n",
       "      <td>11620.7</td>\n",
       "      <td>11620.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18-sep-24</td>\n",
       "      <td>11684.7</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>11720.2</td>\n",
       "      <td>11664.3</td>\n",
       "      <td>11699.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19-sep-24</td>\n",
       "      <td>11778.1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>11808.2</td>\n",
       "      <td>11710.0</td>\n",
       "      <td>11799.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20-sep-24</td>\n",
       "      <td>11753.3</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>11826.7</td>\n",
       "      <td>11747.9</td>\n",
       "      <td>11749.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0        1     2        3        4        5\n",
       "0  16-sep-24  11581.0  0.35  11592.5  11510.0  11520.8\n",
       "1  17-sep-24  11703.4  1.06  11753.9  11620.7  11620.9\n",
       "2  18-sep-24  11684.7 -0.16  11720.2  11664.3  11699.1\n",
       "3  19-sep-24  11778.1  0.80  11808.2  11710.0  11799.5\n",
       "4  20-sep-24  11753.3 -0.21  11826.7  11747.9  11749.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Mostramos las 2 primeras filas del DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Precio</th>\n",
       "      <th>Variación %</th>\n",
       "      <th>Máximo</th>\n",
       "      <th>Mínimo</th>\n",
       "      <th>Apertura</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-sep-24</td>\n",
       "      <td>11581.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>11592.5</td>\n",
       "      <td>11510.0</td>\n",
       "      <td>11520.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17-sep-24</td>\n",
       "      <td>11703.4</td>\n",
       "      <td>1.06</td>\n",
       "      <td>11753.9</td>\n",
       "      <td>11620.7</td>\n",
       "      <td>11620.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Fecha   Precio  Variación %   Máximo   Mínimo  Apertura\n",
       "0  16-sep-24  11581.0         0.35  11592.5  11510.0   11520.8\n",
       "1  17-sep-24  11703.4         1.06  11753.9  11620.7   11620.9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "El número de filas y columnas es: (20, 6)\n"
     ]
    }
   ],
   "source": [
    "# Convertimos la lista de listas que nos devuelve la función, datos_ibex_final, en un Dataframe\n",
    "df_ibex = pd.DataFrame(datos_ibex_final)\n",
    "\n",
    "# Mostramos las 5 primeras filas del DataFrame\n",
    "display(df_ibex.head())\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Ponemos nombre a las columnas usando el método .columns de pandas\n",
    "df_ibex.columns = encabezados_ibex_final\n",
    "\n",
    "\n",
    "# Mostramos las primera fila del DataFrame\n",
    "print(f\"Mostramos las 2 primeras filas del DataFrame:\")\n",
    "display(df_ibex.head(2))\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Vemos el número de filas y de columnas del Dataframe usando el método .shape\n",
    "print(f\"El número de filas y columnas es: {df_ibex.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fecha           object\n",
       "Precio         float64\n",
       "Variación %    float64\n",
       "Máximo         float64\n",
       "Mínimo         float64\n",
       "Apertura       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos los tipos de datos de las columnas\n",
    "df_ibex.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los valores únicos que tenemos en la columna de Fecha son:\n",
      " ['16-sep-24' '17-sep-24' '18-sep-24' '19-sep-24' '20-sep-24' '23-sep-24'\n",
      " '24-sep-24' '25-sep-24' '26-sep-24' '27-sep-24' '30-sep-24' '01-oct-24'\n",
      " '02-oct-24' '03-oct-24' '04-oct-24' '07-oct-24' '08-oct-24' '09-oct-24'\n",
      " '10-oct-24' '11-oct-24']\n",
      "Los valores únicos que tenemos en la columna de Máximo son:\n",
      " [11592.5 11753.9 11720.2 11808.2 11826.7 11836.3 11867.  11856.9 11975.8\n",
      " 12004.6 11971.8 11897.6 11675.5 11664.3 11668.4 11781.2 11734.7 11745.5\n",
      " 11769.6 11729.3]\n"
     ]
    }
   ],
   "source": [
    "# Mostramos algunos valores únicos de nuestro dataframe\n",
    "print(f\"Los valores únicos que tenemos en la columna de Fecha son:\\n {df_ibex['Fecha'].unique()}\")\n",
    "\n",
    "print(f\"Los valores únicos que tenemos en la columna de Máximo son:\\n {df_ibex['Máximo'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos guardar nuestros dataframe en un csv usando el método to_csv\n",
    "# Definimos la ruta completa donde deseamos guardar el archivo HTML\n",
    "ruta_guardado = \"datos/datos_ibex.csv\"\n",
    "\n",
    "with open(ruta_guardado, \"w\", encoding=\"utf-8\") as archivo:\n",
    "    df_ibex.to_csv(ruta_guardado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contenido",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
